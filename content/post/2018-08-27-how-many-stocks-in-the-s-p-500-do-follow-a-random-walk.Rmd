---
title: How many stocks in the S&P 500 do follow a random-walk?
author: Bernardo Reckziegel
date: '2018-08-27'
slug: how-many-stocks-in-the-s-p-500-do-follow-a-random-walk
categories:
  - R
  - finance
tags:
  - manymodels
---



```{r setup, include=FALSE}

knitr::opts_chunk$set(
    message = FALSE, 
    warning = FALSE, 
    eval    = FALSE, 
    cache   = TRUE, 
    echo    = TRUE
)

```


There has been a huge growth of machine learning models’ for finance. The underlying idea behind those models is to put in practice an agnostic search, mostly by brute force, that will deliver forecasts, around some loss function, that are as accuratte the data can tell.

At the same time, forecasters have been spending, over the last 30 years, a huge amount of effort trying to detect veiled patterns in the stock market. Most often than not, with a questionable rate of success. 

As the [Nobel Prize Winner](https://www.nobelprize.org/prizes/economics/2003/granger/facts/), Clive Granger [states](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf), at some sense, the market must follow a random walk, otherwise the it would be an unlimited source of money machine. 

Therefore, is reasonable to ask two questions: how many stocks of the S&P 500 show any evidence of actually following a random-walk? And what implications these results impose for machine learning models in finance? 

Those are the issues I will try to address in this post.


## Load Libraries

```{r}

library(tidyquant)
library(sweep)
library(timetk)
library(tibbletime)
library(forecast)
library(rvest)
library(Quandl)

```



## Download the S&P 500 tickers

After loading the required packages, it’s necessary to get the tickers for all the S&P 500 stocks. These can easily be done by web-scraping the Wikipedia [webpage](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies).

```{r}

get_sp500_tickers <- function() {
    
    raw_get <- function() {
        
        xml2::read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') %>% 
            rvest::html_node('table.wikitable') %>% 
            rvest::html_table() %>% 
            dplyr::as_tibble()
        
    }
    
    possible_get <- purrr::possibly(
        .f        = raw_get, 
        otherwise = NA
    )
    
    possible_get() %>% 
        dplyr::rename(
            ticker_symbol     = 'Ticker symbol', 
            security          = 'Security',
            sec_filings       = 'SEC filings', 
            gics_sector       = 'GICS Sector',
            gics_sub_industry = 'GICS Sub Industry'
        )
    
}

```


`raw_get` funtion is used inside `get_sp500_tickers`. In this way, greater pieces of chunk can be decomposed in smaller fragments and grouped latter on to solve the bigger picture. It's a good practice to use of `possibly()`, from `purrr` to "guarantee" the main script will run. If any error occurs, we get a `NA`, but the code still runs.


```{r}

tickers <- get_sp500_tickers()

tickers

```


```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

tickers

```



## From Tickers to Stocks

The next step is to construct a robust function that will allow us to use all the tickers we got to download the S&P 500 stocks. The function `download_stocks_from_tickers()` is designed to do exactly that and will be used inside the `map()` function from the `purrr` [package](https://purrr.tidyverse.org/).

```{r}

download_stocks_from_tickers <- function(data, ...) {
    
    # tidy eval
    dots_expr <- dplyr::quos(...)
    
    # defensive programming
    possible_quandl <- purrr::possibly(Quandl::Quandl, NA)
    
    # data manipulation
    data %>% 
        dplyr::mutate(possible_download = purrr::map(
            .x = ., 
            .f = ~ possible_quandl(., !!! dots_expr)
        )
    )
        
}

```


```{r}

stocks <- tickers %>% 
    
    # create the list columns to map over
    dplyr::mutate(wiki_tickers = str_c('WIKI/', ticker_symbol, '.11')) %>% 
    tidyr::nest(wiki_tickers) %>%
    
    # download data from 2009 onwards
    dplyr::mutate(download_tickers = map(
        .x           = data, 
        .f           = download_stocks_from_tickers, # the custom function is here!
        order        = 'asc', 
        collapse     = 'monthly', 
        type         = 'raw', 
        start_date   = '2009-01-01', 
        column_index = 11) # column 11 = adjusted close price
        ) %>% 
    
    # reorganize the data in a clean tibble format
    dplyr::select(ticker_symbol, 
                  security, 
                  gics_sector, 
                  gics_sub_industry, 
                  download_tickers
                  ) %>% 
    tidyr::unnest(download_tickers) %>% 
    
    # exclude all the column-lists with NA 
    dplyr::filter(!is.na(possible_download)) %>% 
    
    # cleaning 
    tidyr::unnest(possible_download) %>% 
    dplyr::rename(prices = `Adj. Close`) %>% 
    dplyr::select(Date, 
                  ticker_symbol:gics_sub_industry, 
                  prices
                  ) %>% 
    
    # calculate returns
    dplyr::group_by(ticker_symbol) %>% 
    tidyquant::tq_mutate(
        select     = prices, 
        mutate_fun = periodReturn,
        col_rename = 'returns',
        period     = 'monthly',
        type       = 'log'
        ) %>% 
    dplyr::ungroup()

stocks

```

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks

```


The argument `column_index = 11` ensures that we only get the close prices (adjusted for dividends). This is a courtesy from the [WIKI database](https://www.quandl.com/databases/WIKIP/documentation/about). Stock prices, however, are not stationary and the log-returns were included for future modeling. 


## From `tibble()` to `ts()` 

At this point, we have almost everything is needed to estimate the data generating process (DGP) of S&P 500 stock returns. The next step is to `group_by()` each one of the stocks and coerce them to the `ts` class (remember that the `forecast` [package](http://pkg.robjhyndman.com/forecast/) is built upon the `ts` class). 

```{r}

stocks_to_ts <- stocks %>% 
    dplyr::group_by(ticker_symbol, 
                    security, 
                    gics_sector, 
                    gics_sub_industry
                    ) %>% 
    tidyr::nest(Date, returns) %>% 
    dplyr::mutate(ts = map(
        .x        = data, 
        .f        = timetk::tk_ts,
        start     = 2009,
        frequency = 12)) %>% 
    dplyr::ungroup()

```


```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts

```


## Map Over List Columns

Now the setup is complete. We are now able to use the framework presented in the [chapter 25](http://r4ds.had.co.nz/many-models.html) of **R for data Science**. 

The `auto.arima` function will recursively estimate the most suitable $ARMA$model for each S&P 500 stock. [Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion) is the default selection criteria. The AIC formula is

`$$ AIC = 2k - 2ln(\hat{L}) $$`

in which, `$k$` is the number of parameters in the model and $\hat{L}$ is the minimum value achieved in the likelihood function. With a higher the `$k$`, higher is the penalization and smaller is the chance of a model with more parameters being chosed.

The number of lags in the $p$ and $q$ factor from the $ARMA$ model were limited to be no more than 2. Since the data is monthly, this is identical to assume that shocks at time $t$ are not capable of sistematicaly impact returns by more than $t + 2$ perios ahead (at least from the point of view of statistical significance).  

```{r}

stocks_to_ts_modeling <- stocks_to_ts %>%
    dplyr::group_by(ticker_symbol) %>% 
    dplyr::mutate(model   = map(
        .x         = ts, 
        .f         = forecast::auto.arima, 
        seasonal   = FALSE, 
        stationary = TRUE, 
        max.p      = 2,
        max.q      = 2
    )) %>%
    dplyr::mutate(glance_model = map(
        .x = model, 
        .f = sweep::sw_glance
    )) %>% 
    tidyr::unnest(glance_model) %>% 
    dplyr::ungroup() 

stocks_to_ts_modeling

```

The `stocks_to_ts_modeling` object has standard evaluation metrics, like AIC, BIC, ME, RMSE, MAE, MAPE, MASE, ACF1. Not for one stock, but for all of them. That's how powerful the [tidyverse](https://www.tidyverse.org/) ecosystem can really be!

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling

```

## Final Manipulation and Addicional Toughts

After some extra manipulation we get to the following table:

```{r}

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::mutate(
        ARMA = fct_reorder(ARMA, n), 
        n    = if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models selected by the AIC Criteria', 
        align   = 'c'
    )


```


```{r, echo=FALSE, eval=TRUE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::mutate(
        ARMA = forcats::fct_reorder(ARMA, n), 
        n    = dplyr::if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = forcats::fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models selected by the AIC Criteria', 
        align   = 'c'
    )


```

It’s incredible that 70% of the S&P 500 stocks do not present any sign of predictability (as they are represented by the ARIMA(0, 0, 0) model, which is the random-walk). On top of that, an additional 10% of the stocks are composed by the ARIMA(0, 0, 1), which also follows the $E_t(y_{t + 1}) = E_t(y_{t})$ rule. 

Specially interesting is the fact we have used the AIC criterion for selecting the models. The AIC is known for having a lighter penalization than the [Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). That is, if we had used the BIC criterion as a cutoff, we would probably increase the number of models resembling a random walk, because a fewer parameters would be estimated.

It's easy to see through the use of partial derivatives

`$$ \frac{\partial{AIC}}{\partial{k}} = 2 $$`

and

`$$ \frac{\partial{BIC}}{\partial{k}} = ln(n) $$`

but `$ 2 > ln(n) $` for any `$ n > 7 $`, which is always the case.  

Now, let's come back to the main question: what does Machine Learning has to do with all of these? 

The way that I see is: what the data is telling us, is that we should be very careful before blindly using ML techniques, specially if we care about over-fitting. Whenever you continually unearth into the surface (using parallel computing, non-linear models, etc.) at some point *modeling risk* will came up and get you. In simple terms, model risk means that we are never certain that the selected model is, indeed, a representation of "true" data generating process. 

In the end of the day, we all have to face the fact that financial time series are fabulously noisy, up to a point of being almost completely random. This is the market efficiency in which Granger was [talking about](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf), and there is not much Machine Learning models can do to change that. They can calibrate hyper-parameters, but cannot change the state of the world!


