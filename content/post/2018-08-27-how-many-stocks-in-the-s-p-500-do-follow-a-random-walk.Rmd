---
title: How many stocks in the S&P 500 do follow a random-walk?
author: Bernardo Reckziegel
date: '2018-08-27'
slug: how-many-stocks-in-the-s-p-500-do-follow-a-random-walk
categories:
  - R
  - finance
tags:
  - manymodels
  - randon-walk
  - machine-learning
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(
    message = FALSE, 
    warning = FALSE, 
    eval    = FALSE, 
    cache   = TRUE, 
    echo    = TRUE
)

```


Machine Learning (ML) models are trendy and gaining popularity in finance. Practitioners noticed that little help can come from asymptotic theory if the researcher is constrained by the number of observations (in which $n \rightarrow \infty$ do never happens). 

The solution offered by a great deal of ML models is a complete twist in econometric tradition. Forget about odd features and ugly names, like autocorrelation, multicollinearity and heteroskedasticity. Just pay attention to the forecasts: are they good or not? If not, _make_ them be! 

Still, reality continues to show that it's hard to profit from "hidden" patterns in the stock market. In fact, only a lucky few may know how does it feels like, in the long-run.

As the [Nobel Prize Winner](https://www.nobelprize.org/prizes/economics/2003/granger/facts/), Clive Granger [states](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf): at some sense, the market must follow a random-walk, otherwise the it would be an unlimited source of money machine.

Hard to disagree. On those grounds, I asked myself: how many stocks of the S&P 500 show any evidence of actually following a random-walk process? Is it the number high or low? Think of it: if it's low, no doubt ML models can help a great deal. If not, we are just "drying ice". So.. how do we stand?

I will do my best to address this issue along the way.

## Load Libraries

The required `R` packages for this experiment are:

```{r}

library(tidyquant)
library(sweep)
library(timetk)
library(tibbletime)
library(forecast)
library(rvest)
library(Quandl)

```



## Download the S&P 500 tickers

Once the packages are loaded, we have to start from the scratch. The first step itâ€™s to chase after all S&P 500 tickers. They are not what we really care about, but without them it is hard to move forward. This task can easily be done by web-scraping [Wikipedia's](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) site.


```{r}

get_sp500_tickers <- function() {
    
    raw_get <- function() {
        
        xml2::read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') %>% 
            rvest::html_node('table.wikitable') %>% 
            rvest::html_table() %>% 
            dplyr::as_tibble()
        
    }
    
    possible_get <- purrr::possibly(
        .f        = raw_get, 
        otherwise = NA
    )
    
    possible_get() %>% 
        dplyr::rename(
            ticker_symbol     = 'Ticker symbol', 
            security          = 'Security',
            sec_filings       = 'SEC filings', 
            gics_sector       = 'GICS Sector',
            gics_sub_industry = 'GICS Sub Industry'
        )
    
}

```


In `R`, (i) every object is a function and (ii) every function can be used inside another function. That's why `raw_get` is part of `get_sp500_tickers`. In the `purrr` syntax, we are also allowed to write something like this, without having to same intermediary objects.

```{r, eval=FALSE}
possible_get <- purrr::possibly(
        .f = ~ xml2::read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') %>% 
            rvest::html_node('table.wikitable') %>% 
            rvest::html_table() %>% 
            dplyr::as_tibble(), 
        otherwise = NA
    )
```

The output is the same. `possibly()`, also from `purrr`, "guarantees" the main script will run. If any error occurs, we get a `NA`, but the code still runs. A safe bet for _loops_ with dozens of elements like this one. 


```{r}

tickers <- get_sp500_tickers()

tickers

```


```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

tickers

```


## From Tickers to Stocks

The next step is to construct a function that will allow us to use all the tickers we got to download the companies price history. The function `download_stocks_from_tickers()` will do the work.

```{r}

download_stocks_from_tickers <- function(data, ...) {
    
    # tidy eval
    dots_expr <- dplyr::quos(...)
    
    # defensive programming
    possible_quandl <- purrr::possibly(Quandl::Quandl, NA)
    
    # data manipulation
    data %>% 
        dplyr::mutate(possible_download = purrr::map(
            .x = ., 
            .f = ~ possible_quandl(., !!! dots_expr)
        )
    )
        
}

```


```{r}

stocks <- tickers %>% 
    
    # create the list columns to map over
    dplyr::mutate(wiki_tickers = str_c('WIKI/', ticker_symbol, '.11')) %>% 
    tidyr::nest(wiki_tickers) %>%
    
    # download data from 2009 onwards
    dplyr::mutate(download_tickers = map(
        .x           = data, 
        .f           = download_stocks_from_tickers, # the custom function is here!
        order        = 'asc', 
        collapse     = 'monthly', 
        type         = 'raw', 
        start_date   = '2009-01-01', 
        column_index = 11) # column 11 = adjusted close price
        ) %>% 
    
    # reorganize the data in a clean tibble format
    dplyr::select(ticker_symbol, 
                  security, 
                  gics_sector, 
                  gics_sub_industry, 
                  download_tickers
                  ) %>% 
    tidyr::unnest(download_tickers) %>% 
    
    # exclude all the column-lists with NA 
    dplyr::filter(!is.na(possible_download)) %>% 
    
    # cleaning 
    tidyr::unnest(possible_download) %>% 
    dplyr::rename(prices = `Adj. Close`) %>% 
    dplyr::select(Date, 
                  ticker_symbol:gics_sub_industry, 
                  prices
                  ) %>% 
    
    # calculate returns
    dplyr::group_by(ticker_symbol) %>% 
    tidyquant::tq_mutate(
        select     = prices, 
        mutate_fun = periodReturn,
        col_rename = 'returns',
        period     = 'monthly',
        type       = 'log'
        ) %>% 
    dplyr::ungroup()

stocks

```

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks

```


This data is a courtesy from the [WIKI database](https://www.quandl.com/databases/WIKIP/documentation/about). Of course, getting access to historical prices of 500 different assets may require some time. To speed up things a beat, the argument `column_index = 11` was used to only get the close prices (adjusted by dividends). I'm assuming that other information like the minimum, maximum and opening prices are irrelevant for our purposes.

Finally, `tq_mutate()` is used to calculate stock returns. The adjusted prices are not stationary and shouldn't be used for modeling. [The stationary property of returns make them preferable](https://www.investopedia.com/articles/trading/07/stationary.asp).


## From `tibble()` to `ts()` 

At this point, we have almost everything is needed to estimate the data generating process (DGP) of the S&P 500 stocks. In `R`, the forecasting power-horse is the `forecast` [package](http://pkg.robjhyndman.com/forecast/), build upon the `ts` class.

The script bellow adds this structure into a `tibble` called `stocks_to_ts`. 

```{r}

stocks_to_ts <- stocks %>% 
    dplyr::group_by(ticker_symbol, 
                    security, 
                    gics_sector, 
                    gics_sub_industry
                    ) %>% 
    tidyr::nest(Date, returns) %>% 
    dplyr::mutate(ts = map(
        .x        = data, 
        .f        = timetk::tk_ts,
        start     = 2009,
        frequency = 12)) %>% 
    dplyr::ungroup()

```


```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts

```


## Map Over List-Columns

Now the setup is complete and we are able to use the framework presented in the [chapter 25](http://r4ds.had.co.nz/many-models.html) of **R for data Science**. 

The [`auto.arima`](http://pkg.robjhyndman.com/forecast/reference/auto.arima.html) function will recursively estimate the most suitable $ARMA$ model for each of the S&P 500 stocks. By default, the models are selected according to the [Akaike Selection Criteria (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion). 

The AIC formula (in it's log form) is written as:

$$ AIC = 2k - 2ln( \hat{L} ) $$

in which, $k$ is the number of parameters in the model and $\hat{L}$ is the minimum value achieved in the log-likelihood function. A boost in $k$, increases the penalization and more complex models have as smaller chance of being chosen. 

The number of lags in the $(p,q)$ arguments are limited to be no more than 2. As we will see, this constraint doesn't bind for most stocks, yet serves to speeds up computations greatly. Of course, you can also attach an economic meaning to it: a shock that occurs at time $t$ doesn't impact returns by more than $t + 2$ periods (at least from a probabilistic point of view).    

```{r}

stocks_to_ts_modeling <- stocks_to_ts %>%
    dplyr::group_by(ticker_symbol) %>% 
    dplyr::mutate(model   = map(
        .x         = ts, 
        .f         = forecast::auto.arima, 
        seasonal   = FALSE, 
        stationary = TRUE, 
        max.p      = 2,
        max.q      = 2
    )) %>%
    dplyr::mutate(glance_model = map(
        .x = model, 
        .f = sweep::sw_glance
    )) %>% 
    tidyr::unnest(glance_model) %>% 
    dplyr::ungroup() 

stocks_to_ts_modeling

```

The `stocks_to_ts_modeling` object have standard evaluation metrics, like AIC, BIC, ME, RMSE, MAE, MAPE, MASE, ACF1. Not for one stock, but for all in the S&P 500 index!

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling

```

## Final Manipulation and Addicional Toughts

After some extra manipulation we get to the following table:

```{r}

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::mutate(
        ARMA = fct_reorder(ARMA, n), 
        n    = if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models selected by the AIC Criteria', 
        align   = 'c'
    )


```


```{r, echo=FALSE, eval=TRUE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::mutate(
        ARMA = forcats::fct_reorder(ARMA, n), 
        n    = dplyr::if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = forcats::fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models selected by the AIC Criteria', 
        align   = 'c', 
        format.args = list(width = 15)
    )


```

For 70% of our sample, the $ARIMA(0, 0, 0)$ won the day! (ps: This is the Random-Walk!) On top of that, an additional 10% of total number of stocks are composed by $ARIMA(0, 0, 1)$, who also obeys the $E_t(y_{t + 1}) = E_t(y_{t})$ rule[^1]. The best point forecast for tomorrow is today's return.  

Of course, a different selection criteria could yield contrasting results. But let's keep in mind that $AIC$ actually is not the toughest penalizer. As an example, take the [Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion), that's widely known. If we calculate the partial derivatives with respect to $k$ (the number of model parameters) for both criteria, we get the following[^2]:

$$ \frac{\partial{AIC}}{\partial{k}} = 2 $$

$$ \frac{\partial{BIC}}{\partial{k}} = ln(n) $$

But $ln(n) > 2$, whenever $n > 7$. Our data  definitely has more than 7 observations, so we can be pretty confident that the $BIC$ criterion would hurt overparemetrized models by even more than $AIC$ actually did. That is, $BIC$ would put even more pressure to select simple parsimonious models. In this case, the percentage of random-walks would be grater than we just found. 

To make it clear: returns seems to be so fluky that, for 70% of our sample, not even a single parameter (besides a constant) was worth to attaching. That's the market efficiency in which Granger was [talking about](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf). What can Machine Learning _algos_ "learn" in such a case? 

The quest for accuracy may come at a high cost of a misstated reality.


[^1]: The variance, however, is different. This can lead to some variance-bias trade-off.

[^2]: Someone could argue that $ln(\hat{L})$ and $ln(\sigma^2_e)$ also depends on the number of parameters. That's true, but they only have an indirect effect in $\frac{\partial{AIC}}{\partial{k}}$ and $\frac{\partial{BIC}}{\partial{k}}$ and I'm interested mainly in the direct effects. For this reason, I've omitted $\frac{\partial{AIC}}{\partial{ln(\hat{L})}} \frac{d{ln(\hat{L})}}{d{k}}$ and $\frac{\partial{BIC}}{\partial{ln(\sigma^2_e)}} \frac{d{ln(\sigma^2_e})}{d{k}}$ from the above equations.

