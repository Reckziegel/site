---
title: How many stocks in the S&P 500 do follow a random-walk?
author: Bernardo Reckziegel
date: '2018-08-27'
slug: how-many-stocks-in-the-s-p-500-do-follow-a-random-walk
categories:
  - R
  - finance
tags:
  - manymodels
---



```{r setup, include=FALSE}

knitr::opts_chunk$set(
    message = FALSE, 
    warning = FALSE, 
    eval    = FALSE, 
    cache   = TRUE, 
    echo    = TRUE
)

```


There has been a huge growth of machine learning models’ in finance. The underlying idea behind those models is that a if the data is identically and independent distributed (i.i.d.) the statistician can use the law of large numbers (LLN) and the [Glivenko-Canteli theorem](https://en.wikipedia.org/wiki/Glivenko–Cantelli_theorem) to uncover the asymptotic behavior of the “true” distribution, which is, in fact, unknown. As the number of observations increase, closer and closer we get to the true values.

At the same time, forecasters have been spending, during the last 30 years, a huge amount of time trying to detect hidden patterns in the stock market, most often than not, with a questionable rate of success. As the [Nobel Prize Winner](https://www.nobelprize.org/prizes/economics/2003/granger/facts/), Clive Granger [states](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf), at some sense, the market must follow a random walk, otherwise the it would be an unlimited source of money machine. So, two questions emerge: how many stocks of the S&P 500 show any evidence of following a random-walk? And what implications the results impose for machine learning models in finance? 

Those are the issues I will try to address in this post.


## Load Libraries

```{r}

library(tidyquant)
library(sweep)
library(timetk)
library(tibbletime)
library(forecast)
library(rvest)
library(Quandl)

```



## Download the S&P 500 tickers

After loading the required packages, it’s necessary to download the tickers for all the S&P 500 stocks. These can easily be done by web-scraping the Wikipedia [webpage](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies).

```{r}

get_sp500_tickers <- function() {
    
    raw_get <- function() {
        
        xml2::read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies') %>% 
            rvest::html_node('table.wikitable') %>% 
            rvest::html_table() %>% 
            dplyr::as_tibble()
        
    }
    
    possible_get <- purrr::possibly(
        .f        = raw_get, 
        otherwise = NA
    )
    
    possible_get() %>% 
        dplyr::rename(
            ticker_symbol     = 'Ticker symbol', 
            security          = 'Security',
            sec_filings       = 'SEC filings', 
            gics_sector       = 'GICS Sector',
            gics_sub_industry = 'GICS Sub Industry'
        )
    
}

```


I've used a function inside another function because I believe that this is one of the biggest assets of working with `R`: great chunks of code can be decomposed in small pieces and then grouped together latter on to solve the bigger picture. Specially important is the use of `possibly()` that "guarantees" that the main script will run. If any error occurs, we get `NA` as an answer, but the code still runs.


```{r}

tickers <- get_sp500_tickers()

tickers

```


```{r, eval=TRUE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

tickers

```



## From tickers to stocks

The next step is to construct a robust function that will allow us to use all the tickers above to download the S&P 500 stocks. The function `download_stocks_from_tickers()` is designed to do exactly that and will be used inside the `map()` function from the `purrr` [package](https://purrr.tidyverse.org/).

```{r}

download_stocks_from_tickers <- function(data, ...) {
    
    # tidy eval
    dots_expr <- dplyr::quos(...)
    
    # defensive programming
    possible_quandl <- purrr::possibly(Quandl::Quandl, NA)
    
    # data manipulation
    data %>% 
        dplyr::mutate(possible_download = purrr::map(
            .x = ., 
            .f = ~ possible_quandl(., !!! dots_expr)
        )
    )
        
}

```


```{r}

stocks <- tickers %>% 
    
    # create the list columns to map over
    dplyr::mutate(wiki_tickers = str_c('WIKI/', ticker_symbol, '.11')) %>% 
    tidyr::nest(wiki_tickers) %>%
    
    # download data from 2009 onwards
    dplyr::mutate(download_tickers = map(
        .x           = data, 
        .f           = download_stocks_from_tickers, # the custom function is here!
        order        = 'asc', 
        collapse     = 'monthly', 
        type         = 'raw', 
        start_date   = '2009-01-01', 
        column_index = 11) # column 11 = adjusted close price
        ) %>% 
    
    # reorganize the data in a clean tibble format
    dplyr::select(ticker_symbol, 
                  security, 
                  gics_sector, 
                  gics_sub_industry, 
                  download_tickers
                  ) %>% 
    tidyr::unnest(download_tickers) %>% 
    
    # exclude all the column-lists with NA 
    dplyr::filter(!is.na(possible_download)) %>% 
    
    # cleaning 
    tidyr::unnest(possible_download) %>% 
    dplyr::rename(prices = `Adj. Close`) %>% 
    dplyr::select(Date, 
                  ticker_symbol:gics_sub_industry, 
                  prices
                  ) %>% 
    
    # calculate returns
    dplyr::group_by(ticker_symbol) %>% 
    tidyquant::tq_mutate(
        select     = prices, 
        mutate_fun = periodReturn,
        col_rename = 'returns',
        period     = 'monthly',
        type       = 'log'
        ) %>% 
    dplyr::ungroup()

stocks

```

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks

```


To avoid downloading unnecessary data the argument `column_index = 11`, which only contains the adjusted prices, was used in conjunction with `download_stocks_from_tickers()`.  The data was downloaded from the free [WIKI database](https://www.quandl.com/databases/WIKIP/documentation/about) in Quandl.

In some cases, may be that we cannot get the data through the provided API. In this occurs, `possibly()` will return a `NA` and the command ` filter(!is.na(possible_download))` will wipe out those rows from the dataset. Finally, using the known fact that stock prices are not stationary, the returns were calculated and included for future modeling. 


## From `tibble()` to `ts()` 

At this point, we have almost everything is needed to estimate the data generating process (DGP) of S&P 500 stock returns. The next step is to `group_by()` each one of the stocks and coerce them to the `ts` class (remember that the `forecast` [package](http://pkg.robjhyndman.com/forecast/) is built upon the `ts` class). 

```{r}

stocks_to_ts <- stocks %>% 
    dplyr::group_by(ticker_symbol, 
                    security, 
                    gics_sector, 
                    gics_sub_industry
                    ) %>% 
    tidyr::nest(Date, returns) %>% 
    dplyr::mutate(ts = map(
        .x        = data, 
        .f        = timetk::tk_ts,
        start     = 2009,
        frequency = 12)) %>% 
    dplyr::ungroup()

```


```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts

```


## Map over list columns

Now the setup is complete and we are able to use the framework presented in the [chapter 25](http://r4ds.had.co.nz/many-models.html) of **R for data Science**. The `map()` function recursively estimates which ARMA model fits better in which one of the stocks according to [Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion) selection criterion. The AIC formula is

`$$ AIC = 2k - 2ln(\hat{L}) $$`

in which `$k$`is the number of parameters in the model. The higher the `$k$`, higher the penalization and smaller the change of an overparametrized model being chose.

I also limited the number of lags in the p and q factor to be no more than 2. Since the data is organized on a monthly period, I’m also assuming that today's random shocks are not capable of disturb stock returns by more than two months ahead (at least, from a statistical significant point of view).  

```{r}

stocks_to_ts_modeling <- stocks_to_ts %>%
    dplyr::group_by(ticker_symbol) %>% 
    dplyr::mutate(model   = map(
        .x         = ts, 
        .f         = forecast::auto.arima, 
        seasonal   = FALSE, 
        stationary = TRUE, 
        max.p      = 2,
        max.q      = 2
    )) %>%
    dplyr::mutate(glance_model = map(
        .x = model, 
        .f = sweep::sw_glance
    )) %>% 
    tidyr::unnest(glance_model) %>% 
    dplyr::ungroup() 

stocks_to_ts_modeling

```

In the last two lines of the printed tibble we see that the `stocks_to_ts_modeling` object now has all the standard evaluation metrics, like AIC, BIC, ME, RMSE, MAE, MAPE, MASE, ACF1. Not for one stock, but for all of them. The [tidyverse](https://www.tidyverse.org/) is power!

```{r, eval=TRUE, echo=FALSE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling

```

## Final Manipulation and addicional toughts

After some extra manipulation we get to the following result

```{r}

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, as_factor) %>% 
    dplyr::mutate(
        ARMA = fct_reorder(ARMA, n), 
        n    = if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models fitted according to AIC Criterion', 
        align   = 'c'
    )


```


```{r, echo=FALSE, eval=TRUE}

load("C:/Users/Berna/Desktop/R Programming/site/rw_script_dataset.Rdata")

stocks_to_ts_modeling %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::count(model.desc, sort = TRUE) %>% 
    tidyr::separate(
        data = ., 
        col  = model.desc, 
        into = c('ARMA', 'drift'), 
        sep  = ' ') %>% 
    dplyr::select(ARMA, n) %>% 
    dplyr::mutate_if(is.character, forcats::as_factor) %>% 
    dplyr::mutate(
        ARMA = forcats::fct_reorder(ARMA, n), 
        n    = dplyr::if_else(ARMA == lag(ARMA), n + lag(n), n)
        ) %>% 
    dplyr::filter(!is.na(n)) %>% 
    dplyr::mutate(percent = n / sum(n)) %>% 
    dplyr::mutate(ARMA = forcats::fct_reorder(ARMA, percent)) %>%
    knitr::kable(
        digits  = 2,
        caption = 'ARMA models fitted according to AIC Criterion', 
        align   = 'c'
    )


```

It’s incredible that 70% of the S&P 500 stocks do not present any sign of predictability (they are represented by the ARIMA(0, 0, 0) model, which is a random-walk). On top of that, an additional 10% of the stocks are composed by the ARIMA(0, 0, 1), which also follows the $E_t(y_t) = E_t(y_{t-1})$ rule. 

Specially interesting is the fact we have used the AIC criterion for selecting the models, which is known for having a lighter penalization than the [Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). That is, if we had used the BIC criterion as a cutoff, we would probably increase the number of models resembling a random walk, because a fewer parameters would be estimated.

But... What Machine Learning has to do with all of these? What the data is telling us, is that we should be very careful before blindly using ML techniques, specially if we care about over-fitting. If you keep continually digging into the surface (using parallel computing, non-linear models, etc.) at some point you will face *modeling risk*. In simple terms, model risk means that we are never certain that the selected model is, indeed, a representation of "true" data generating process. 

In the end of the day, we all have to face the fact that financial time series are fabulously noisy, up to a point of being almost completely random. This is the market efficiency in which Granger was [talking about](http://www.forecastingprinciples.com/paperpdf/Granger-stockmarket.pdf), and there is not much Machine Learning models can do to change that. They can calibrate hyper-parameters, but cannot change the state of the world!


