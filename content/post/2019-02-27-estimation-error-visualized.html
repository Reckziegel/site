---
title: Estimation Error Visualized
author: Bernardo Reckziegel
date: '2019-02-27'
slug: estimation-error-visualized
categories:
  - estimation
  - finance
  - R
tags:
  - shrinkage
---



<p>The objective of economic models is to dress common-sense into mathematical formalism<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. For some economic models, however, real data is too scare to achieve good statistical analysis. This scarcity of data leads to estimation-error<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> (the econometricians enemy number one).</p>
<p>By estimation-error I mean: the researcher is never certain about the <em>true</em> data generating process (DGP) of the phenomenon at hand. As Allan Timmermann and Graham Elliott argue, economic models can be thought as a condense representation of reality, in which mispecifification flourish naturally<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To circumvent this problem statisticians appeal basically for two methods: <em>robust statistics</em> and <em>shrinkage</em>.</p>
<p>The idea of robust statistics is to make data less sensitive to outliers. “Robust” means that small changes in data (or in the way data is described), do not affect by much it’s descriptive parameters. In that sense, the median is a robust statistic, while the mean it’s not.</p>
<p>Shrinkage, on the other hand, is a way of averaging different estimators. In this case, practitioners often combine parametric and non-parametric estimators in attempt to keep the best of both options.</p>
<p>One interesting application of shrinkage methods is in the estimation of variance-covariance matrices. Those matrices are required by almost any mathematical method in economics: from portfolio optimization to least squares and its strands (GLS, GMM, etc).</p>
<p>A problem arises, though, if the matrix dimension, <span class="math inline">\(p\)</span>, is large in comparison to the number of observations, <span class="math inline">\(n\)</span>. If <span class="math inline">\(p &gt; n\)</span> the matrix is not even invertible. A less extreme case would be given by a <span class="math inline">\(p\)</span> that approaches <span class="math inline">\(n\)</span>. In these scenario, the sample covariance matrix is invertible, but numerically ill-conditioned, which means that the inverse matrix can amplify estimation error dramatically<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>.</p>
<p>To illustrate this point more clearly, let’s simulate a series of matrices with increasing length. For each length, the eigenvalues (<span class="math inline">\(\lambda\)</span>) will be calculated to check if there is any association between <span class="math inline">\(n\)</span> (the number of time series length) and the matrix dimension <span class="math inline">\(p\)</span> (think of <span class="math inline">\(p\)</span> as he number of assets in a stock universe, for example). For this exercise, <span class="math inline">\(p = 50\)</span>, while <span class="math inline">\(n\)</span> grows from <span class="math inline">\(50\)</span>, to <span class="math inline">\(100\)</span>, to <span class="math inline">\(150\)</span>, …, up to <span class="math inline">\(1.000\)</span>. For each <span class="math inline">\(n\)</span>, <span class="math inline">\(100\)</span> simulations will be run to compute the average eigenvalues. Finally, the random shocks will be generated by a standard multivariate normal distribution, with zero mean and an unitary variance, <span class="math inline">\(\epsilon \sim N(0, 1)\)</span>.</p>
<p>Why are the eigenvalues important? Think on them as a <em>proxy</em> for how much information the data contains. In the realm of optimization, for example, the eigenvalues can be used to check the maximum (or minimum) condition of a stationary point. In differential equations, they are used to access the rate of convergence around a certain steady-state. When the eigenvalues are negative<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>, the process if convergent; if any is positive, the process is explosive.</p>
<p>That said, we would expect the eigenvalues of matrices of high <span class="math inline">\(p\)</span> and low <span class="math inline">\(n\)</span> to be unstable and more uncertain, while they stabilize as <span class="math inline">\(n\)</span> grows indefinitely.</p>
<pre class="r"><code>library(mvtnorm)
library(tidyverse) 

# covariance matrix dimension
cov_dimension &lt;- 50  

# different time series lenght
n             &lt;- seq(cov_dimension, 20 * cov_dimension, cov_dimension) 

# number of simulations to compute for each ts_lenght
simulations   &lt;- 100 

# mean 0
mu            &lt;- rep(0, times = cov_dimension)

# unitary variance
sigma         &lt;- diag(cov_dimension)

# Compute sample eigenvalues from time series of different lenght
lambda_fitted  &lt;- matrix(
    data = 0,
    nrow = length(n),
    ncol = cov_dimension
    ) %&gt;%
    `rownames&lt;-`(n)

# for each time series lenght
for (k in seq_along(n)) {

    t         &lt;- n[k]
    eigen_aux &lt;- 0

    # compute 100 simulations
    for (j in seq(simulations)) {

        # simulate the time series
        sample_covariance &lt;-

            # epsilon
            rmvnorm(n = t, mean = mu, sigma = sigma) %&gt;%

            # sample covariance
            cov()

        eigen_values &lt;- sample_covariance %&gt;%

            # eigevectors and eigenvalues
            eigen() %&gt;%

            # extract only the eigenvalues
            .$`values` %&gt;%

            # order by size
            sort()

        # and sum all their values
        eigen_aux &lt;- eigen_aux + eigen_values

    }

    # average of eigenvalues across different scenarios
    eigen_aux &lt;- rev(eigen_aux / simulations)

    # store the resulting average eigenvalues
    lambda_fitted[k, ] &lt;- eigen_aux

}</code></pre>
<p>The object <code>n</code> contains the length of each simulated time series.</p>
<pre class="r"><code>n</code></pre>
<pre><code>##  [1]   50  100  150  200  250  300  350  400  450  500  550  600  650  700
## [15]  750  800  850  900  950 1000</code></pre>
<p>The <code>for loop</code> above can be better understood as:</p>
<ul>
<li>for each <span class="math inline">\(n\)</span>
<ul>
<li>calculate 100 simulations
<ul>
<li>generate random numbers</li>
<li>estimate the sample covariance</li>
<li>estimate the eigenvalues</li>
<li>sum them all</li>
</ul></li>
</ul></li>
<li>average by the number of simulations</li>
<li>reorder from the highest to the lowest</li>
<li>store their values in separate rows</li>
</ul>
<p>The <code>lambda_fitted</code> object contain the estimated eigenvalues we really care about:</p>
<pre class="r"><code>lambda_fitted[1:5, 1:5]</code></pre>
<pre><code>##         [,1]     [,2]     [,3]     [,4]     [,5]
## 50  3.773481 3.415792 3.112939 2.903928 2.713163
## 100 2.767573 2.558064 2.417292 2.285636 2.162859
## 150 2.375210 2.223898 2.111311 2.007136 1.924859
## 200 2.153245 2.033587 1.944844 1.863487 1.784688
## 250 2.028657 1.917432 1.834543 1.762906 1.704360</code></pre>
<p>In which I printed only the first <span class="math inline">\(5\)</span> rows and columns to save space. As we look from the left to the right, the simulated eigenvalues start to decrease. That’s nothing wrong with that since I have ordered them this way. The problem, thought, is that they are more variable at the top-left of the matrix then they are at the bottom-right.</p>
<p>A 2-D surface clearly shows the distortion effect caused by estimation-error.</p>
<pre class="r"><code>library(gganimate)
library(latex2exp)

# data manipulation
lambda_fitted_tidy &lt;- lambda_fitted %&gt;%
    as.data.frame() %&gt;%
    rownames_to_column(var = &#39;ts_size&#39;) %&gt;% 
    as_tibble() %&gt;%
    gather(key, value, -ts_size) %&gt;%
    mutate(key = str_remove(key, &#39;V&#39;)) %&gt;%
    mutate_if(is_character, as.double) 

# plot 
plot &lt;- lambda_fitted_tidy %&gt;%
    ggplot(aes(x = ts_size, y = value, color = key, group = key)) +
    geom_density_2d(show.legend = FALSE) +
    scale_color_viridis_c() + 
    scale_y_continuous(limits = c(-0.5, 4)) + 
    scale_x_continuous(limits = c(-200, 1300)) + 
    geom_hline(yintercept = 0, color = &#39;white&#39;, show.legend = FALSE) + 
    labs(
        title    = &quot;Sample Variance-Covariance Matrix Simulation&quot;,
        subtitle = &quot;Sorted eigenvalues: from highest to lowest&quot;,
        x        = &quot;Time Series Length&quot;,
        y        = TeX(&quot;$\\lambda$&quot;)
    ) +
    theme(element_blank()) + 

    # dynamics
    transition_states(key, transition_length = 1, state_length = 1)


animate(plot)</code></pre>
<p><img src="/post/2019-02-27-estimation-error-visualized_files/figure-html/unnamed-chunk-4-1.gif" style="display: block; margin: auto;" /></p>
<p>The mean value of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(1\)</span>, which is in accordance with the multivariate process of <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span> generated above.</p>
<p>If there were no estimation error, the distribution of <span class="math inline">\(\lambda\)</span> should be essentially flat (since the DGP is the same for all <span class="math inline">\(n\)</span>). But as we see, when data is short the ellipse twists dramatically. It’s in this scenario that “shrinkage” comes in. The ill-behavior of covariance matrices are alleviated by exploring the bias &amp; variance trade-off of a highly structured matrix and one with little or no structure at all. The end result is a flatter and more precise ellipse around it’s <em>true</em> eigenvalues (as it should be).</p>
<p>The simulation above reveals a fast decay in the average eigenvalues within <span class="math inline">\(500\)</span> and <span class="math inline">\(750\)</span> realizations, after when they stabilize. This give us some clue on the minimum amount of data we should have when dealing with matrices of <span class="math inline">\(p = 50\)</span>.</p>
<pre class="r"><code>lambda_fitted_tidy %&gt;% 
    group_by(ts_size) %&gt;% 
    summarise(variance = var(value)) %&gt;% 
    ggplot(aes(x = ts_size, y = variance, color = ts_size)) + 
    geom_point(size = 3, show.legend = FALSE) + 
    geom_line(show.legend = FALSE) + 
    theme(element_blank()) + 
    scale_color_viridis_c() + 
    labs(
        title    = &quot;Eigenvalues Variance&quot;,
        subtitle = &quot;Conditional on the Number of Observations&quot;,
        x        = &quot;Time Series Length&quot;, 
        y        = TeX(&quot;$\\sigma ^ 2$&quot;)
    )</code></pre>
<p><img src="/post/2019-02-27-estimation-error-visualized_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>This is in accordance with the empirical application I explored in previous posts (see <a href="https://www.bernardo.codes/2018/09/05/how-much-shrinkage-does-the-stock-market-requires/">here</a> and <a href="https://www.bernardo.codes/2018/09/08/a-covariance-matrix/">here</a>).</p>
<p>The lesson is: when in doubt, always shrink!</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“Economic Rules”, Dani Rodrik.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Of course, if the model is wrong, not even with tons of data can help.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See in <a href="https://rady.ucsd.edu/docs/faculty/timmermann/forecasting-in-economics-and-finance.pdf">Forecasting in Economics and Finance</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>See “A Well-Conditioned Estimator For Large-Dimensional Covariance Matrices”, from Oliver Ledoit and Michael Wolf. Link <a href="http://ledoit.net/ole1a.pdf">here</a>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>If the eigenvalues are complex numbers than a sufficient condition is given by the real part being negative.<a href="#fnref5">↩</a></p></li>
</ol>
</div>
