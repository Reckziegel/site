---
title: Estimation Error Visualized
author: Bernardo Reckziegel
date: '2019-02-27'
slug: estimation-error-visualized
categories:
  - estimation
  - finance
  - R
tags:
  - shrinkage
---



<p>The objective of economic models is to dress common-sense into mathematical formalism<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. For some economic problems, however, data is too scare to achieve good statistical analysis. This scarcity of data leads to estimation-error<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> (the econometricians enemy number one).</p>
<p>By estimation-error I mean: the researcher is never certain about the <em>true</em> data generating process (DGP) of the phenomenon at hand. As Allan Timmermann and Graham Elliott argue, economic models can be thought as a condense representation of reality, in which mispecifification flourish naturally<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To circumvent this problem statisticians appeal basically for two methods: robust statistics and shrinkage.</p>
<p>The idea of robust statistics is to make data less sensitive to outliers. “Robust” means that small changes in data (or in the way data is described), do not affect (by much) it’s descriptive parameters. In that sense, the median is a robust statistic, while the mean it’s not.</p>
<p>Shrinkage, on the other hand, is a way of averaging different estimators. In this case, practitioners often combine parametric and non-parametric estimators in attempt to keep the best of both options.</p>
<p>One interesting application of shrinkage methods is in the estimation of variance-covariance matrices. Those matrices are required by almost any mathematical method in economics: from portfolio optimization to least squares and its strands (GLS, GMM, etc).</p>
<p>A problem arises, though, if the matrix dimension, <span class="math inline">\(p\)</span>, is large in comparison to the number of observations, <span class="math inline">\(n\)</span>. If <span class="math inline">\(p &gt; n\)</span> the matrix is not even invertible. A less extreme case would be given by a <span class="math inline">\(p\)</span> that approaches <span class="math inline">\(n\)</span>. In those scenarios, the sample covariance matrix is invertible, but numerically ill-conditioned, which means that the inverse matrix can amplify estimation error dramatically<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>.</p>
<p>To illustrate this point more clearly, let’s simulate a series of matrices with increasing length. For each length, the eigenvalues (<span class="math inline">\(\lambda\)</span>) will be calculated to check if there is any association between <span class="math inline">\(n\)</span> (the number of time series lenght) and the matrix dimension <span class="math inline">\(p\)</span> (think of <span class="math inline">\(p\)</span> as he number of assets in a stock universe, for example). For this exercise, <span class="math inline">\(p = 50\)</span>, while <span class="math inline">\(n\)</span> grows from <span class="math inline">\(50\)</span>, to <span class="math inline">\(100\)</span>, to <span class="math inline">\(150\)</span>, …, up to <span class="math inline">\(1.000\)</span>. For each <span class="math inline">\(n\)</span>, <span class="math inline">\(100\)</span> simulations will be run to compute the average eigenvalues. Finally, the random shocks will be generated by a standard multivariate normal distribution, with zero mean and an unitary variance, <span class="math inline">\(N(0, 1)\)</span>.</p>
<p>Why are the eigenvalues so important? Think on them as a <em>proxy</em> for how much information the data contains. In the realm of optimization, for example, the eigenvalues can be used to check the maximum (or minimum) condition of a stationary point. In differential equations, they are used to access the rate of convergence around a certain steady-state. If the eigenvalues are negative, the process if convergent; if any is positive, the process is explosive.</p>
<p>That said, we would expect the eigenvalues of matrices of high <span class="math inline">\(p\)</span> and low <span class="math inline">\(n\)</span> to be unstable and more uncertain, while they stabilize as <span class="math inline">\(n\)</span> grows indefinetly.</p>
<pre class="r"><code>library(mvtnorm)
library(tidyverse) 

cov_dimension &lt;- 50  # covariance matrix dimension
n             &lt;- seq(cov_dimension, 20 * cov_dimension, cov_dimension) # different time series lenght
simulations   &lt;- 100 # number of simulations to compute for each ts_lenght
mu            &lt;- rep(0, times = cov_dimension)
sigma         &lt;- diag(cov_dimension)

# Compute sample eigenvalues from time series of different lenght
lambda_fitted  &lt;- matrix(
    data = 0,
    nrow = length(n),
    ncol = cov_dimension
    ) %&gt;%
    `rownames&lt;-`(n)

# for each time series lenght
for (k in seq_along(n)) {

    t         &lt;- n[k]
    eigen_aux &lt;- 0

    # compute 100 simulations
    for (j in seq(simulations)) {

        # simulate the time series
        sample_covariance &lt;-

            # epsilon
            rmvnorm(n = t, mean = mu, sigma = sigma) %&gt;%

            # sample covariance
            cov()

        eigen_values &lt;- sample_covariance %&gt;%

            # eigevectors and eigenvalues
            eigen() %&gt;%

            # extract only the eigenvalues
            .$`values` %&gt;%

            # order by size
            sort()

        # and sum all their values
        eigen_aux &lt;- eigen_aux + eigen_values

    }

    # average of eigenvalues across different scenarios
    eigen_aux &lt;- rev(eigen_aux / simulations)

    # store the resulting average eigenvalues
    lambda_fitted[k, ] &lt;- eigen_aux

}</code></pre>
<p>The object <code>n</code> contains the length of each simulated time series.</p>
<pre class="r"><code>n</code></pre>
<pre><code>##  [1]   50  100  150  200  250  300  350  400  450  500  550  600  650  700
## [15]  750  800  850  900  950 1000</code></pre>
<p>The <code>for loop</code> above can be better understood as:</p>
<ul>
<li>for each <span class="math inline">\(n\)</span>
<ul>
<li>calculate 100 simulations
<ul>
<li>generate random numbers</li>
<li>estimate the sample covariance</li>
<li>estimate the eigenvalues</li>
<li>sum them all</li>
</ul></li>
</ul></li>
<li>average by the number of simulations</li>
<li>reorder from the higher to the lowest</li>
<li>store their values in separate rows</li>
</ul>
<p>The <code>lambda_fitted</code> object contain the estimated eigenvalues we care about:</p>
<pre class="r"><code>lambda_fitted[1:5, 1:5]</code></pre>
<pre><code>##         [,1]     [,2]     [,3]     [,4]     [,5]
## 50  3.760982 3.418733 3.136979 2.905294 2.710886
## 100 2.795685 2.576825 2.402808 2.269093 2.159951
## 150 2.374259 2.224858 2.119207 2.020761 1.930674
## 200 2.180811 2.047186 1.948808 1.865522 1.794770
## 250 2.024284 1.918845 1.837133 1.769952 1.703346</code></pre>
<p>In wich I printed only the <span class="math inline">\(5\)</span> first rows and columns to save space. As we look from the left to the right, the simulated eigenvalues start to decrease. That’s nothing wrong with that since I have ordered them this way. The problem, thought, is that they are more variable at the top-left then they are at the bottom-right.</p>
<p>A 2-D distribution surface clearly shows the distortion effect caused by estimation-error.</p>
<pre class="r"><code>library(gganimate)
library(latex2exp)

plot &lt;- lambda_fitted %&gt;%

    # data manipulation
    as.data.frame() %&gt;%
    rownames_to_column(var = &#39;ts_size&#39;) %&gt;% 
    as_tibble() %&gt;%
    gather(key, value, -ts_size) %&gt;%
    mutate(key = str_remove(key, &#39;V&#39;)) %&gt;%
    mutate_if(is_character, as.double) %&gt;% 

    # plot
    ggplot(aes(x = ts_size, y = value, color = key, group = key)) +
    geom_density_2d(show.legend = FALSE) +
    scale_color_viridis_c() + 
    scale_y_continuous(limits = c(-0.5, 4)) + 
    scale_x_continuous(limits = c(-200, 1300)) + 
    labs(
        title    = &quot;Variance-Covariance Matrix Simulation&quot;,
        subtitle = &quot;Sorted eigenvalues: from the highest to the lowest&quot;,
        x        = &quot;Time Series Length&quot;,
        y        = TeX(&quot;$\\lambda$&quot;),
        caption  = &#39;Source: bernardo.codes&#39;
    ) +

    # dynamics
    transition_states(key, transition_length = 1, state_length = 1)


animate(plot)</code></pre>
<p><img src="/post/2019-02-27-estimation-error-visualized_files/figure-html/unnamed-chunk-4-1.gif" style="display: block; margin: auto;" /></p>
<p>The mean value of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(1\)</span>, which is in accordance with the multivariate process of <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span> generated above.</p>
<p>The ill-behavior of covariance matrices can be corrected by shrinkage methods, as I showed in previous posts (see <a href="https://www.bernardo.codes/2018/09/05/how-much-shrinkage-does-the-stock-market-requires/">here</a> and <a href="https://www.bernardo.codes/2018/09/08/a-covariance-matrix/">here</a>).</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“Economic Rules”, Dani Rodrik.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Of course, if the model is wrong, not even with tons of data can help.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See in <a href="https://rady.ucsd.edu/docs/faculty/timmermann/forecasting-in-economics-and-finance.pdf">Forecasting in Economics and Finance</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>See “A Well-Conditioned Estimator For Large-Dimensional Covariance Matrices”, from Oliver Ledoit and Michael Wolf. Link <a href="http://ledoit.net/ole1a.pdf">here</a>.<a href="#fnref4">↩</a></p></li>
</ol>
</div>
