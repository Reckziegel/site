---
title: Estimation Error Visualized
author: Bernardo Reckziegel
date: '2019-02-27'
slug: estimation-error-visualized
categories:
  - estimation
  - finance
  - R
tags:
  - shrinkage
---

The objective of economic models is to dress common-sense into mathematical formalism[^1]. For some economic problems, however, data is too scare to achieve good statistical analysis. This scarcity of data leads to estimation-error[^2] (the econometricians enemy number one). 

By estimation-error I mean: the researcher is never certain about the _true_ data generating process (DGP) of the phenomenon at hand. As Allan Timmermann and Graham Elliott argue, economic models can be thought as a condense representation of reality, in which mispecifification flourish naturally[^3]. 

To circumvent this problem statisticians appeal basically for two methods: robust statistics and shrinkage. 

The idea of robust statistics is to make data less sensitive to outliers. "Robust" means that small changes in data (or in the way data is described), do not affect (by much) it's descriptive parameters. In that sense, the median is a robust statistic, while the mean it's not. 

Shrinkage, on the other hand, is a way of averaging different estimators. In this case, practitioners often combine parametric and non-parametric estimators in attempt to keep the best of both options.

One interesting application of shrinkage methods is in the estimation of variance-covariance matrices. Those matrices are required by almost any mathematical method in economics: from portfolio optimization to least squares and its strands (GLS, GMM, etc). 

A problem arises, though, if the matrix dimension, $p$, is large in comparison to the number of observations, $n$. If $p > n$ the matrix is not even invertible. A less extreme case would be given by a $p$ that approaches $n$. In those scenarios, the sample covariance matrix is invertible, but numerically ill-conditioned, which means that the inverse matrix can amplify estimation error dramatically[^4]. 

To illustrate this point more clearly, let's simulate a series of matrices with increasing length. For each length, the eigenvalues ($\lambda$) will be calculated to check if there is any association between $n$ (the number of time series lenght) and the matrix dimension $p$ (think of $p$ as he number of assets in a stock universe, for example). For this exercise, $p = 50$, while $n$ grows from $50$, to $100$, to $150$, ..., up to $1.000$. For each $n$, $100$ simulations will be run to compute the average eigenvalues. Finally, the random shocks will be generated by a standard multivariate normal distribution, with zero mean and an unitary variance, $N(0, 1)$. 

Why are the eigenvalues so important? Think on them as a _proxy_ for how much information the data contains. In the realm of optimization, for example, the eigenvalues can be used to check the maximum (or minimum) condition of a stationary point. In differential equations, they are used to access the rate of convergence around a certain steady-state. If the eigenvalues are negative, the process if convergent; if any is positive, the process is explosive. 

That said, we would expect the eigenvalues of matrices of high $p$ and low $n$ to be unstable and more uncertain, while they stabilize as $n$ grows indefinetly. 

```{r, message=FALSE, warning=FALSE}

library(mvtnorm)
library(tidyverse) 

cov_dimension <- 50  # covariance matrix dimension
n             <- seq(cov_dimension, 20 * cov_dimension, cov_dimension) # different time series lenght
simulations   <- 100 # number of simulations to compute for each ts_lenght
mu            <- rep(0, times = cov_dimension)
sigma         <- diag(cov_dimension)

# Compute sample eigenvalues from time series of different lenght
lambda_fitted  <- matrix(
    data = 0,
    nrow = length(n),
    ncol = cov_dimension
    ) %>%
    `rownames<-`(n)

# for each time series lenght
for (k in seq_along(n)) {

    t         <- n[k]
    eigen_aux <- 0

    # compute 100 simulations
    for (j in seq(simulations)) {

        # simulate the time series
        sample_covariance <-

            # epsilon
            rmvnorm(n = t, mean = mu, sigma = sigma) %>%


            # sample covariance
            cov()

        eigen_values <- sample_covariance %>%

            # eigevectors and eigenvalues
            eigen() %>%

            # extract only the eigenvalues
            .$`values` %>%

            # order by size
            sort()

        # and sum all their values
        eigen_aux <- eigen_aux + eigen_values

    }

    # average of eigenvalues across different scenarios
    eigen_aux <- rev(eigen_aux / simulations)

    # store the resulting average eigenvalues
    lambda_fitted[k, ] <- eigen_aux

}

```

The object `n` contains the length of each simulated time series. 

```{r}
n
```

The `for loop` above can be better understood as:

- for each $n$
    - calculate 100 simulations 
        - generate random numbers
        - estimate the sample covariance
        - estimate the eigenvalues
        - sum them all
- average by the number of simulations
- reorder from the higher to the lowest
- store their values in separate rows

The `lambda_fitted` object contain the estimated eigenvalues we care about:

```{r}
lambda_fitted[1:5, 1:5]
```

In wich I printed only the $5$ first rows and columns to save space. As we look from the left to the right, the simulated eigenvalues start to decrease. That's nothing wrong with that since I have ordered them this way. The problem, thought, is that they are more variable at the top-left then they are at the bottom-right.  

A 2-D distribution surface clearly shows the distortion effect caused by estimation-error. 

```{r, message=FALSE, warning=FALSE, fig.align='center', fig.width=14}

library(gganimate)
library(latex2exp)

plot <- lambda_fitted %>%

    # data manipulation
    as.data.frame() %>%
    rownames_to_column(var = 'ts_size') %>% 
    as_tibble() %>%
    gather(key, value, -ts_size) %>%
    mutate(key = str_remove(key, 'V')) %>%
    mutate_if(is_character, as.double) %>% 

    # plot
    ggplot(aes(x = ts_size, y = value, color = key, group = key)) +
    geom_density_2d(show.legend = FALSE) +
    scale_color_viridis_c() + 
    scale_y_continuous(limits = c(-0.5, 4)) + 
    scale_x_continuous(limits = c(-200, 1300)) + 
    labs(
        title    = "Variance-Covariance Matrix Simulation",
        subtitle = "Sorted eigenvalues: from the highest to the lowest",
        x        = "Time Series Length",
        y        = TeX("$\\lambda$"),
        caption  = 'Source: bernardo.codes'
    ) +

    # dynamics
    transition_states(key, transition_length = 1, state_length = 1)


animate(plot)

```

The mean value of $\lambda$ is $1$, which is in accordance with the multivariate process of $\mu=0$ and $\sigma^2 = 1$ generated above. 

The ill-behavior of covariance matrices can be corrected by shrinkage methods, as I showed in previous posts (see [here](https://www.bernardo.codes/2018/09/05/how-much-shrinkage-does-the-stock-market-requires/) and [here](https://www.bernardo.codes/2018/09/08/a-covariance-matrix/)).


[^1]: "Economic Rules", Dani Rodrik.

[^2]: Of course, if the model is wrong, not even with tons of data can help.

[^3]: See in [Forecasting in Economics and Finance](https://rady.ucsd.edu/docs/faculty/timmermann/forecasting-in-economics-and-finance.pdf).

[^4]: See "A Well-Conditioned Estimator For Large-Dimensional Covariance Matrices", from Oliver Ledoit and Michael Wolf. Link [here](http://ledoit.net/ole1a.pdf).
