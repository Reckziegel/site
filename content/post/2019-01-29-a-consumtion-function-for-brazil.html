---
title: A consumption function for Brazil
author: Bernardo Reckziegel
date: '2019-01-29'
slug: a-consumtion-function-for-brazil
categories:
  - Macroeconomics
tags:
  - OLS
  - Instrumental Variables
  - Consumption
---



<p>The Keynesian consumption function is one of the most famous equations in macroeconomics. This post will show how it can be estimated and how we can use a simple trick to add dynamics into the system.</p>
<div id="a-functional-form-for-c_t" class="section level2">
<h2>A functional form for <span class="math inline">\(C_t\)</span></h2>
<p>A traditional representation of consumption is</p>
<p><span class="math display">\[ C_t = \alpha + \beta_0 Y^d_t \]</span></p>
<p>in which, <span class="math inline">\(C_t\)</span> is consumption at time <span class="math inline">\(t\)</span>, <span class="math inline">\(Y^d_t\)</span> is the disposable income at <span class="math inline">\(t\)</span> and <span class="math inline">\(\beta_0\)</span> is the <em>marginal propensity to consume</em> (MPC).</p>
<p>This last parameter has a straightforward interpretation: if the disposable income increases by one unit, consumption increases by <span class="math inline">\(\beta_0\)</span> units of <span class="math inline">\(Y^d\)</span>.</p>
<p>In theory, the adjustment occurs immediately. On reality, though, it is quite probable the adjustment requires some time to occur.</p>
<p>A more realistic interpretation of this phenomenon would be given by the following stochastic format</p>
<p><span class="math display">\[ C_t = \alpha + \beta_0 Y_t + \beta_1 Y_{t-1} + ... + \beta_k Y_{t-k} + u_t \]</span> We now have a whole bunch of multipliers. <span class="math inline">\(\beta_0\)</span> is the short-term impact of <span class="math inline">\(Y_t\)</span> in <span class="math inline">\(C_t\)</span>, <span class="math inline">\(\beta_0 + \beta_1\)</span> measures an intermediate impact in <span class="math inline">\(C_t\)</span>, so on so forth. The long-run impact is given by <span class="math inline">\(\sum_{i=0}^{k} B_i\)</span>, which is the sum of all the different impacts that <span class="math inline">\(\beta_i\)</span> has in <span class="math inline">\(C_t\)</span>.</p>
<p>OK, we have the dynamics… but how many lags do we choose?</p>
</div>
<div id="a-nice-approach" class="section level2">
<h2>A Nice Approach</h2>
<p>The trick here is to assume that <span class="math inline">\(\beta&#39;s\)</span> decline geometrically. If they do, they are tied in time by</p>
<p><span class="math display">\[ \beta_k = \beta_0 \lambda ^ k \]</span> for <span class="math inline">\(k = 0, 1, 2, ...\)</span></p>
<p>The equation it is just saying the more recent the data is, the more relevant is to explain the current changes in <span class="math inline">\(C_t\)</span>. In finance, for example, analysts frequently use the <a href="https://www.investopedia.com/articles/07/ewma.asp">EWMA estimator</a>, which explores the same idea.</p>
<p>When</p>
<ul>
<li><span class="math inline">\(k = 0 \rightarrow \beta_0 \approx \beta_0\)</span></li>
<li><span class="math inline">\(k = 1 \rightarrow \beta_1 \approx \beta_0 \lambda\)</span></li>
<li><span class="math inline">\(k = 2 \rightarrow \beta_2 \approx \beta_0 \lambda ^ 2\)</span></li>
<li><span class="math inline">\(...\)</span></li>
<li><span class="math inline">\(k = n \rightarrow \beta_n \approx \beta_0 \lambda ^ n\)</span></li>
</ul>
<p>Since we have defined the <span class="math inline">\(\beta&#39;s\)</span> as a geometric sequence, their limiting behavior is known and converge to</p>
<p><span class="math display">\[\lim_{k \to \infty} = \beta_0 \bigg( \frac{1}{1 - \lambda} \bigg)\]</span> Writing the consumption function using the relations derived above yields</p>
<p><span class="math display">\[ C_t = \alpha + \beta_0 Y_t + \beta_0 \lambda Y_{t-1} + \beta_0 \lambda^2 Y_{t-2} + ... + \beta_0 \lambda^k Y_{t-k} + u_t \]</span> What is valid for <span class="math inline">\(C_t\)</span> must also valid for <span class="math inline">\(C_{t-1}\)</span> and we are allowed to rewrite <span class="math inline">\(C\)</span> as</p>
<p><span class="math display">\[ C_{t-1} = \alpha + \beta_0 Y_{t-1} + \beta_0 \lambda Y_{t-2} + \beta_0 \lambda^2 Y_{t-3} + ... + \beta_0 \lambda^k Y_{t-k-1} + u_{t-1} \]</span> The elegant move comes from the subtraction of <span class="math inline">\(C_t\)</span> by <span class="math inline">\(\lambda C_{t-1}\)</span>, that is: <span class="math inline">\(C_t - \lambda C_{t-1}\)</span>. This algebra leads to</p>
<p><span class="math display">\[ C_t = \alpha(1 - \lambda) + \beta_0 Y_t + \lambda C_{t-1} + v_t \]</span> in which <span class="math inline">\(v_t = u_t - \lambda u_{t-1}\)</span>.</p>
<p><em>Voilà</em>! A process that appeared to be quite complicated is now greatly simplified!</p>
<p>The equation above shows the short-them response of <span class="math inline">\(C_t\)</span> for an increase in <span class="math inline">\(Y_t\)</span>. More importantly, even if we know that the effect of <span class="math inline">\(Y\)</span> in <span class="math inline">\(C\)</span> occur with lags, the whole adjustment path can be recovered by <span class="math inline">\(C_{t-1}\)</span> and the follow dependencies</p>
<ul>
<li><span class="math inline">\(t=0 \rightarrow \beta_0 \approx \beta_0\)</span></li>
<li><span class="math inline">\(t=1 \rightarrow \beta_1 \approx \beta_0 \lambda\)</span></li>
<li><span class="math inline">\(t=2 \rightarrow \beta_2 \approx \beta_0 \lambda ^2\)</span></li>
<li><span class="math inline">\(t=3 \rightarrow \beta_3 \approx \beta_0 \lambda ^3\)</span></li>
<li><span class="math inline">\(...\)</span></li>
</ul>
<p>The long-term relationship, on the other hand, can be retrieved dividing the right-hand side of <span class="math inline">\(C_t\)</span> by <span class="math inline">\((1 - \lambda)\)</span> and excluding <span class="math inline">\(C_{t-1}\)</span> from the equation (equilibrium definition implies <span class="math inline">\(C_t = C_{t-1}\)</span> in long-run).</p>
<p>Finally, the most important feature of this <em>ad hoc</em> approach is the connection with the <a href="https://www.investopedia.com/terms/a/adaptiveexpthyp.asp">Adaptative Expectations Hypothesis</a>.</p>
</div>
<div id="adaptative-expectations" class="section level2">
<h2>Adaptative Expectations</h2>
<p>Assume that individuals change their consumption behavior only if their are confident that changes they face are permanent. If this is not the case, switches are short-lived. In this universe, a long-lasting stir would only be sustained by long-run movements of the state of the economy (a new equilibrium condition), not by current noise. <a href="https://www.nobelprize.org/prizes/economic-sciences/1976/friedman/facts/">Milton Friedman</a> was the first to raise this point in great detail (see <a href="https://press.princeton.edu/titles/978.html">here</a>).</p>
<p>A theoretical model to test this proposition should look more or less like</p>
<p><span class="math display">\[ C_t = \alpha + \beta_0 Y^*_t + u_t \]</span> in which <span class="math inline">\(Y^*\)</span> is the long-run output and <span class="math inline">\(u_t\)</span> is a <em>white-noise</em> disturbance.</p>
<p>Since <em><span class="math inline">\(Y^*\)</span> is non-observable</em>, let’s infer, as a starting point, that individuals can (and do) learn from their experiences. A parsimonious way to incorporate this behavior is by setting</p>
<p><span class="math display">\[ Y^*_t - Y^*_{t-1} = \gamma (Y_t - Y^*_{t-1}) \]</span> which is identical to <span class="math inline">\(Y^*_t = \gamma Y_t + (1 -\gamma) Y^*_{t-1}\)</span>.</p>
<p>The equilibrium output is a weighted averaged of the current output and the first lag of its long-term (equilibrium) value. In this case, <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(1 - \gamma\)</span> act as weights.</p>
<p>Substituting <span class="math inline">\(Y^*_t\)</span> in <span class="math inline">\(C_t\)</span> and collecting the terms produce</p>
<p><span class="math display">\[ C_t = \gamma \alpha + \gamma \beta_0 Y_t + (1 - \gamma) C_{t-1} + v_t \]</span> in which <span class="math inline">\(v_t = u_t - (1 - \gamma) u_{t-1}\)</span>.</p>
<p><em>This virtually identical to the equation derived before assuming a geometric decay for <span class="math inline">\(\beta\)</span></em>!</p>
</div>
<div id="a-theorical-note" class="section level2">
<h2>A Theorical Note</h2>
<p>The derivation of Ordinary Least Squares (OLS) adds a restriction on the residuals behavior: they should not be correlated with any of the explanatory variables. As a consequence, the autocorrelation between <span class="math inline">\(v_t\)</span> and <span class="math inline">\(v_{t-1}\)</span> is null (or close to zero).</p>
<p>Unfortunately, that’s not the case for our model. Note:</p>
<p><span class="math display">\[
\begin{align}
Cov(v_t, v_{t-1}) &amp;= \big[u_t - (1 - \gamma) u_{t-1} \big] \big[u_{t-1} - (1 - \gamma) u_{t-2} \big] \\
Cov(v_t, v_{t-1}) &amp;= -(1 - \gamma) u_{t-1}^2 \\
Cov(v_t, v_{t-1}) &amp;= -(1 - \gamma) \sigma ^2 
\end{align}
\]</span></p>
<p>The autocorrelation does not sum to zero. The result is identical for <span class="math inline">\(C_{t-1}\)</span> and <span class="math inline">\(v_t\)</span>:</p>
<p><span class="math display">\[
\begin{align}
Cov(C_{t-1}, v_t) &amp;= \big [C_{t-1} \big] \big[u_{t} - (1 - \gamma) u_{t-1} \big] \\
Cov(C_{t-1}, v_t) &amp;= \big [\gamma \alpha + \gamma \beta_0 Y_{t-1} + (1 - \gamma) C_{t-2} + u_{t-1} - (1 - \gamma) u_{t-2} \big] \big[u_{t} - (1 - \gamma) u_{t-1} \big] \\
Cov(C_{t-1}, v_t) &amp;= -(1 - \gamma) u_{t-1}^2  \\
Cov(C_{t-1}, v_t) &amp;= -(1 - \gamma) \sigma ^2 
\end{align}
\]</span></p>
<p>Therefore, <span class="math inline">\(C_{t-1}\)</span>, although predetermined, is not fully exogenous. As a result, it’s not possible to rule out, without further investigation, the possibility of biased and inconsistent estimators (whether or not this is the case, however, is an empirical question).</p>
<p>Fortunately, this obstacle can be addressed by the <a href="https://en.wikipedia.org/wiki/Instrumental_variables_estimation">instrumental variables (IV)</a> approach.</p>
<p>In the following session we shall use both (OLS and IV).</p>
</div>
<div id="empirical-estimation-of-c_t" class="section level2">
<h2>Empirical Estimation of <span class="math inline">\(C_t\)</span></h2>
<p>Let’s assume that the <em>true</em> specification of <span class="math inline">\(C_t\)</span> is given by <span class="math inline">\(C_t = \alpha + \beta_0 Y^*_t + u_t\)</span> with adaptive expectations.</p>
<p>From the steps derived above we have to estimate</p>
<p><span class="math display">\[ C_t = \alpha(1 - \lambda) + \beta_0 Y_t + \lambda C_{t-1} + v_t \]</span> Let’s do that step-by-step using <a href="https://www.rstudio.com/">RStudio</a>.</p>
<div id="load-libraries" class="section level3">
<h3>1. Load libraries</h3>
<p>The required libraries for this exercise are</p>
<pre class="r"><code>library(tidyverse)   # loads the tidyverse
library(timetk)      # tidy time manipulation
library(tibbletime)  # tidy time manipulation
library(sidrar)      # download data from IBGE
library(seasonal)    # seasonal decomposition
library(lmtest)      # robust standard errors and IV estimation
library(sandwich)    # robust stardard errors
library(strucchange) # stability tests</code></pre>
<p>If you don’t have any of them installed yet, just run <code>install.packages('desired package')</code> into your console. All libraries are available at <a href="https://cran.r-project.org/">CRAN</a>.</p>
</div>
<div id="get-the-data" class="section level3">
<h3>2. Get the data</h3>
<p>The following data are needed:</p>
<ul>
<li>Consumption</li>
<li>GDP net of taxes (a <em>proxy</em> for disposable income)</li>
<li>Consumer Inflation Index (IPCA)</li>
</ul>
<p>The choice for first two is obvious. The last one, however, has a different purpose: is used to get rid of distortionary effects caused by inflation. The period covered will go from 2003 up to 3Q of 2018 (last data available).</p>
<p>All the data will be downloaded from <a href="https://ww2.ibge.gov.br/english/">IBGE</a>, the official provider of brazilian national accounts. <em>Consumption</em>, <em>GDP</em> and <em>Taxes</em> are computed on a quarterly basis, while consumer inflation is reported monthly. The cumulative inflation over three months will be used to make easier to aggregate the data from a frequency of 12 to 4 periods (we just have to select the 3, 6, 9 and 12 observations of each year). With the quarterly cumulative inflation index at hand, deflate <span class="math inline">\(C\)</span> and <span class="math inline">\(Y^d\)</span> is painless.</p>
<p>The package <a href="https://cran.r-project.org/web/packages/sidrar/index.html">sidrar</a> is used for the downloads (vignette <a href="https://cran.r-project.org/web/packages/sidrar/vignettes/Introduction_to_sidrar.html">here</a>).</p>
<hr />
<p><strong>A note:</strong> It’s possible to calculate <span class="math inline">\(Y^d\)</span> from brazilian national accounts, but it’s not an easy task given the high level of government transfers and “unconventional” taxation methods adopted by official authorities (see <a href="http://www.brazil-help.com/taxes.htm">here</a>). As so, <span class="math inline">\(Y - T\)</span> is used as a <em>proxy</em> for <span class="math inline">\(Y^d\)</span>.</p>
<hr />
<pre class="r"><code># consumption
consumption &lt;- get_sidra(
    api = &#39;/t/1846/n1/all/v/all/p/last%2063/c11255/93404/d/v585%200&#39;
    ) %&gt;% 
    as_tibble() %&gt;% 
    select(consumption = Valor)

# gdp
gdp &lt;- get_sidra(
    api = &#39;/t/1846/n1/all/v/all/p/last%2063/c11255/90707/d/v585%200&#39;
    ) %&gt;% 
    as_tibble() %&gt;% 
    select(gdp = Valor)

# taxes
taxes &lt;- get_sidra(
    api = &#39;/t/1846/n1/all/v/all/p/last%2063/c11255/90706/d/v585%200&#39;
    ) %&gt;% 
    as_tibble() %&gt;% 
    select(taxes = Valor)

# inflation
ipca &lt;- get_sidra(
    api = &#39;/t/1737/n1/all/v/2263/p/all/d/v2263%202&#39;
    ) %&gt;% 
    as_tibble() %&gt;% 
    separate(col = &#39;Mês&#39;, into = c(&#39;month&#39;, &#39;year&#39;)) %&gt;% 
    mutate(year = as.numeric(year)) %&gt;% 
    filter(month %in% c(&#39;março&#39;, &#39;junho&#39;, &#39;setembro&#39;, &#39;dezembro&#39;), year &gt;= 2003) %&gt;% 
    slice(-nrow(.)) %&gt;% 
    select(ipca = Valor)</code></pre>
</div>
<div id="dessazonalize-and-deflate" class="section level3">
<h3>3. Dessazonalize and Deflate</h3>
<p>Those are “raw” objects and, not by surprise, exhibit a strong seasonal component (evidence omitted to save space). There are different ways of dealing with this issue and I think that, at least among economists, the most used technique is X-13 ARIMA-SEATS. This is the same procedure adopted by <a href="https://www.census.gov/srd/www/x13as/">US Census Bureau</a>. I will cherish the tradition among my peers by using it as well.</p>
<p>The seasonally adjusted data have very different magnitudes. To make them more comparable, the year of 2003 is chosen as base year after the series being deflated. The base year is an index starting at 100.</p>
<p>The script bellow show how it is done.</p>
<pre class="r"><code>data_clean &lt;- 
    
    # add dates
    create_series(&#39;2003&#39; ~ &#39;2018-09&#39;, period = &#39;quarterly&#39;) %&gt;% 
    
    # bind all the series
    bind_cols(consumption, gdp, taxes, ipca / 100 + 1) %&gt;% 
    
    # tidy thing
    gather(key = key, value = value, -date) %&gt;% 
    mutate_if(is_character, as_factor) %&gt;%
    group_by(key) %&gt;%
    nest(date, value) %&gt;%
    
    # seasonal package only operates with the ts class
    mutate(map(
        .x = data, 
        
        # coerce to ts
        .f = ~ tk_ts(
            data      = .x, 
            start     = c(2003, 1), 
            end       = c(2018, 3), 
            frequency = 4, 
            silent = TRUE
        ) %&gt;% 
            
            # then apply the seasonal filtering
            seas() %&gt;% 
            
            # extract the fitted values
            final() %&gt;% 
            
            # and bring back to the tidy framework
            tk_tbl() %&gt;% 
            rename(value_seats = value)
        )
    ) %&gt;% 
    unnest() %&gt;% 
    select(date, key, everything(), -index, -value) %&gt;% 
    
    spread(key, value_seats) %&gt;% 
    
    # deflate
    mutate(
        consumption = consumption / ipca, 
        gdp         = gdp / ipca, 
        taxes       = taxes / ipca, 
        y_d         = gdp - taxes,
        
        # 2003 as a base year   
        consumption   = (consumption / consumption[[1]]) * 100, 
        y_d           = (y_d / y_d[[1]]) * 100
        ) %&gt;% 
    select(date, consumption, y_d) 

data_clean</code></pre>
<pre><code>## # A time tibble: 63 x 3
## # Index: date
##    date                consumption   y_d
##    &lt;dttm&gt;                    &lt;dbl&gt; &lt;dbl&gt;
##  1 2003-01-01 00:00:00        100   100 
##  2 2003-04-01 00:00:00        104.  105.
##  3 2003-07-01 00:00:00        105.  109.
##  4 2003-10-01 00:00:00        107.  112.
##  5 2004-01-01 00:00:00        110.  117.
##  6 2004-04-01 00:00:00        113.  120.
##  7 2004-07-01 00:00:00        118.  122.
##  8 2004-10-01 00:00:00        122.  125.
##  9 2005-01-01 00:00:00        124.  129.
## 10 2005-04-01 00:00:00        128.  134.
## # ... with 53 more rows</code></pre>
</div>
<div id="fit-ols" class="section level3">
<h3>4. Fit OLS</h3>
<p>With a clean <code>tibble</code> at hand let’s jump straight to OLS estimation:</p>
<pre class="r"><code>model_ols &lt;- data_clean %&gt;% 
    lm(consumption ~ y_d + lag(consumption, 1), data = .) 

model_ols %&gt;% 
    coeftest(x = ., vcov. = sandwich::vcovHAC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##                      Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)         -1.018853   0.751110 -1.3565    0.1801    
## y_d                  0.321349   0.037253  8.6260 4.821e-12 ***
## lag(consumption, 1)  0.692202   0.036364 19.0351 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Except for the intercept, all the estimated coefficients have the expected sign. They are also highly significant, as can be seen by the calculated t-statistic (the <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">HAC</a> estimator has already corrected the standard errors for autocorrelation and heteroskedasticity). From the regression we find</p>
<p><span class="math display">\[
\begin{align}
\alpha(1 - \lambda) &amp;= -1.018853 \\
\beta_0             &amp;= 0.321349  \\
\lambda             &amp;= 0.692202  \\
\end{align}
\]</span></p>
<p>The short-term impact of a positive one-unit shock in the GDP (net of taxes) is of the magnitude of <span class="math inline">\(0.33(=\beta_0)\)</span>. In the period that follow, the shock is still in effect and have an impact of <span class="math inline">\(\beta_o \lambda\)</span>, in <span class="math inline">\(t = 2\)</span>; <span class="math inline">\(\beta_o \lambda ^ 2\)</span>, in <span class="math inline">\(t = 3\)</span>; so on so forth.</p>
<p>The short and the long-term equations are rewritten as</p>
<p><span class="math display">\[ 
\begin{align}
C_t &amp;= -1.02 + 0.32 Y^d_t + 0.70 C_{t-1} &amp; \text{(short-term)} \\
C_t &amp;= -3.31 + 1.04 Y^d_t                &amp; \text{(long-term)} 
\end{align}
\]</span> Notice the difference in <em>marginal propensity to consume</em> as we allow the time window to expand. In the long-run, a positive unit shock in <span class="math inline">\(Y^d\)</span> translates entirely into new consumption.</p>
<p>Since this process is a geometric sequence, the average adjustment can be calculated as</p>
<p><span class="math display">\[ \text{Avg}_{ols} = \frac{\lambda}{1 - \lambda} = \frac{0.692202}{1 - 0.692202} \approx 2.25 \text{ quarters}\]</span> which is a reasonably fast transition.</p>
<p>Against the initial expectations, there is no evidence of autocorrelation up to 12 lags, according to Breusch-Godfrey Test:</p>
<pre class="r"><code>data_clean %&gt;% 
    bgtest(formula = consumption ~ y_d + lag(consumption, 1), 
           order   = 12, 
           type    = &#39;F&#39;, 
           data    = ., 
           fill    = 100) </code></pre>
<pre><code>## 
##  Breusch-Godfrey test for serial correlation of order up to 12
## 
## data:  consumption ~ y_d + lag(consumption, 1)
## LM test = 0.28772, df1 = 12, df2 = 47, p-value = 0.9886</code></pre>
<p>Finally, the fitted parameters appears to be stable and don’t cross the barriers of the CUSUM test.</p>
<pre class="r"><code>data_clean %&gt;% 
    efp(formula = consumption ~ y_d + lag(consumption, 1), 
        data    = ., 
        type    = &#39;OLS-CUSUM&#39;, 
        dynamic = FALSE) %&gt;% 
    plot()</code></pre>
<p><img src="/post/2019-01-29-a-consumtion-function-for-brazil_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="fit-iv" class="section level3">
<h3>5. Fit IV</h3>
<p>The IV estimation requires a two step procedure: (i) run <span class="math inline">\(C_t = \theta_0 + \theta_1 Y^d_t\)</span> and save <span class="math inline">\(\hat{C}_{t}\)</span>; (ii) run <span class="math inline">\(C_t = \alpha + \beta Y^d_t + \Theta \hat{C}_{t-1}\)</span>, in which <span class="math inline">\(\hat{C}_{t}\)</span> is now “purified” from it’s improper correlation with <span class="math inline">\(v_t\)</span>.</p>
<pre class="r"><code>model_iv &lt;- data_clean %&gt;% 
    mutate(iv = lm(consumption ~ y_d, data = .)$fitted) %&gt;% 
    lm(consumption ~ y_d + lag(iv, 1), data = .)

model_iv %&gt;% 
    coeftest(x = ., vcov. = sandwich::vcovHAC)</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  0.12810    3.68172  0.0348   0.97236    
## y_d          0.26677    0.14869  1.7942   0.07791 .  
## lag(iv, 1)   0.74384    0.14222  5.2301 2.344e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>All the coefficients have the expected sign (including the intercept). The fitted parameter for <span class="math inline">\(Y^d\)</span> is lower than the OLS method and the lagged value for <span class="math inline">\(\hat{C}_{t-1}\)</span> is higher, which may indicate that shocks (when occur) dissipate a at a slower rate.</p>
<p>The regression analysis imply</p>
<p><span class="math display">\[ 
\begin{align}
C_t &amp;= 0.13 + 0.27 Y^d_t + 0.74 C_{t-1} &amp; \text{(short-term)} \\
C_t &amp;= 0.50 + 1.04 Y^d_t                &amp; \text{(long-term)} 
\end{align}
\]</span> The long-term effect is nearly identical, whereas the short-term responses differs somehow. The average impulse has increased from <span class="math inline">\(2.25\)</span> to <span class="math inline">\(\lambda / (1 - \lambda) = 2.90\)</span> quarters.</p>
<p>The current model contains autocorrelation (tested using Breusch-Godfrey procedure up to 12 lags), which may not be a severe problem in this case, since the fitted parameters do not deviate much from our previous model.</p>
<p>As an exercise, let’s incorporate the error term into the IV equation, lagged by one period. By doing this, we find:</p>
<pre class="r"><code>model_iv_lag_error &lt;- data_clean %&gt;% 
    mutate(
        iv = lm(consumption ~ y_d, data = .)$fitted, 
        v  = lm(consumption ~ y_d, data = .)$residuals
        ) %&gt;% 
    lm(consumption ~ y_d + lag(iv, 1) + lag(v, 1), data = .) 

model_iv_lag_error %&gt;% 
    coeftest(x = ., vcov. = sandwich::vcovHAC) </code></pre>
<pre><code>## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) -2.750755   1.300027 -2.1159   0.03866 *  
## y_d          0.434902   0.077380  5.6203 5.707e-07 ***
## lag(iv, 1)   0.581351   0.075371  7.7132 1.855e-10 ***
## lag(v, 1)    0.760947   0.049344 15.4214 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The fitted coefficient for <span class="math inline">\(Y^d\)</span> has gone up from <span class="math inline">\(0.27\)</span> (OLS) to <span class="math inline">\(0.43\)</span>. That’s a huge change. All the variables are statistically significant at 95% confidence interval.</p>
<p>The behavior equations are</p>
<p><span class="math display">\[ 
\begin{align}
C_t &amp;= -2.75 + 0.43 Y^d_t + 0.58 C_{t-1} &amp; \text{(short-term)} \\
C_t &amp;= -6.57 + 1.04 Y^d_t                &amp; \text{(long-term)} 
\end{align}
\]</span> This IV representation appears to be well specified and passes all tests applied so far. The Breusch-Godfrey test (12 lags) has a p-value of <span class="math inline">\(0.98\)</span> (not shown here to save space). As so, the null-hypothesis of no autocorrelation can’t be rejected. The OLS-CUSUM points in the same direction by showing stability in the coefficients.</p>
<pre class="r"><code>data_clean %&gt;% 
    mutate(
        iv = lm(consumption ~ y_d, data = .)$fitted, 
        v  = lm(consumption ~ y_d, data = .)$residuals
    ) %&gt;% 
    efp(formula = consumption ~ y_d + lag(iv, 1) + lag(v, 1), 
        data    = ., 
        type    = &#39;OLS-CUSUM&#39;, 
        dynamic = FALSE) %&gt;% 
    plot()</code></pre>
<p><img src="/post/2019-01-29-a-consumtion-function-for-brazil_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="a-graphical-interpretation" class="section level2">
<h2>A Graphical Interpretation</h2>
<p>Let’s use the objects created above to visualize the impact of a <span class="math inline">\(Y^d_t\)</span> shock in <span class="math inline">\(C_t\)</span>.</p>
<pre class="r"><code>tibble(
    Time = 1:12, 
    OLS  = model_ols$coefficients[[2]] * ((model_ols$coefficients[[3]]) ^ Time) / model_ols$coefficients[[3]], 
    IV = model_iv$coefficients[[2]] * ((model_iv$coefficients[[3]]) ^ Time) / model_iv$coefficients[[3]], 
    `IV with lag error` = model_iv_lag_error$coefficients[[2]] * ((model_iv_lag_error$coefficients[[3]]) ^ Time) / model_iv_lag_error$coefficients[[3]]
    ) %&gt;% 
    
    # tidy
    gather(key = &quot;Models&quot;, value = &quot;Shock&quot;, -Time, factor_key = TRUE) %&gt;% 
    
    # plot
    ggplot(aes(x = Time, y = Shock, color = Models)) +
    geom_line(size = 1) +
    scale_colour_viridis_d() + 
    labs(title    = &#39;Effect of a Temporary Shock in Consumption&#39;, 
         subtitle = &#39;Period 2003-2018. Quarterly data.&#39;, 
         y        = &#39;Impact (%)&#39;, 
         caption  = &quot;Source: IBGE and Brazilian National Accounts.&quot;) + 
    theme_minimal() + 
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2019-01-29-a-consumtion-function-for-brazil_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>It clearly stands out that the IV Model (with lag error) manifests a higher short-term impact then the rest of the models. At the same time, the shocks estimated with this specification seems to decay faster then the rest. After two years (8 quarters), only a minor fraction of previous disturbances still affect the consumption path. This result is valid for all models.</p>
<p>From the equations above it is also possible to draw a graph of <span class="math inline">\(C-Y^d\)</span> in the two dimensional hyperplane.</p>
<pre class="r"><code>sequence &lt;- seq(from = 100, to = 500, by = 10)

list(
    tibble(
        seq   = sequence,
        fit   = −1.02 + 0.32 * sequence,
        model = &#39;OLS&#39;, 
        Type  = &#39;Short-Run&#39;
    ), 
    tibble(
        seq   = sequence,
        fit   = −3.31 + 1.04 * sequence,
        model = &#39;OLS&#39;, 
        Type  = &#39;Long-Run&#39;
    ), 
    tibble(
        seq   = sequence,
        fit   = −0.13 + 0.27 * sequence,
        model = &#39;IV&#39;, 
        Type  = &#39;Short-Run&#39;
    ),
    tibble(
        seq   = sequence,
        fit   = 0.50  + 1.04 * sequence,
        model = &#39;IV&#39;, 
        Type  = &#39;Long-Run&#39;
    ),
    tibble(
        seq   = sequence,
        fit   = −2.75 + 0.43 * sequence,
        model = &#39;IV With Lag Error&#39;, 
        Type  = &#39;Short-Run&#39;
    ),
    tibble(
        seq   = sequence,
        fit   = -6.57 + 1.04 * sequence,
        model = &#39;IV With Lag Error&#39;, 
        Type  = &#39;Long-Run&#39;
    )
) %&gt;% 
    reduce(bind_rows) %&gt;% 
    mutate_if(is_character, as_factor) %&gt;% 
    ggplot(aes(x = seq, y = fit, color = Type)) +
    facet_wrap(~model, scales = &#39;free_y&#39;) + 
    geom_line(size = 1) + 
    scale_colour_viridis_d() + 
    theme_minimal() + 
    theme(legend.position = &quot;bottom&quot;) + 
    labs(title    = &#39;Brazilian Consumption Function&#39;,
         subtitle = &#39;Period of 2003-2018. Quarterly data.&#39;,
         x = &#39;Disposable Income&#39;, 
         y = &#39;Consumption&#39;, 
         caption = &quot;Source: IBGE and Brazilian National Accounts.&quot;)</code></pre>
<p><img src="/post/2019-01-29-a-consumtion-function-for-brazil_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>In this post three different specifications were discussed to explain the brazilian consumption path. In agreement with the theory (see <a href="https://www.amazon.com/Advanced-Macroeconomics-Mcgraw-hill-Economics-David/dp/1260185214/ref=sr_1_1?ie=UTF8&amp;qid=1550107391&amp;sr=8-1&amp;keywords=advanced+macroeconomics+romer">Romer</a> chapter 8), the relationship between consumption and available income is flatter in the short-run and stepper for longer periods of time. There is also some uncertainty around the right parameter for <span class="math inline">\(Y^d\)</span>, that lies in between <span class="math inline">\(\{0.27, 0.43\}\)</span>.</p>
<p>As we give time for shocks to fully accommodate, a unit change in <span class="math inline">\(Y^d\)</span> implies an impact of 1-to-1 in <span class="math inline">\(C\)</span>. The average adjustment occurs relatively fast, around 2 or 3 quarters. Taken together, the results indicate there is a low propensity to forgo consumption, given increases in income. In technical terms: <em>the intertemporal elasticity of substituition appears to be low</em>.</p>
<p>It’s possible to expand this study in many ways. The inclusion of interest-rates may help to improve the results, since interest-rates can change the relative prices of consumption among the present and future. Credit as a fraction of the GDP, unemployment, local exchange-rate, etc. may also add new insights. All of those variables can be found at the Brazilian Central Bank (BCB) or IBGE webpages. Additional care with endogenity issues is never too much and always advisable.</p>
</div>
