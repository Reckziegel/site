---
title: A consumption function for Brazil
author: Bernardo Reckziegel
date: '2019-01-29'
slug: a-consumtion-function-for-brazil
categories:
  - Macroeconomics
tags:
  - OLS
  - Instrumental Variables
  - Consumption
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)   # loads the tidyverse
library(timetk)      # tidy time manipulation
library(tibbletime)  # tidy time manipulation
library(sidrar)      # get data from IBGE  
library(seasonal)    # seasonal decomposition
library(strucchange) # stability tests
library(lmtest)      # robust standard errors
library(sandwich)    # robust stardard errors

```


The Keynesian consumption function is one of the most famous equations in macroeconomics. This post will show how it can be estimated and how can we use a simple trick to add dynamics into the system.

## A functional form for $C_t$

A traditional representation of consumption is

$$ C_t = \alpha + \beta_0 Y^d_t $$

in which, $C_t$ is consumption at time $t$,  $Y^d_t$ is the disposable income at $t$ and $\beta_0$ is the _marginal propensity to consume_ (MPC). 

This last parameter has a straightforward meaning: if the disposable income increases by one unit, consumption increases by $\beta_0$ units of $Y^d$. 

In theory, the adjustment occurs immediately. On reality, though, it is quite probable the adjustment requires some time to occur.  

A more realistic interpretation of this phenomenon would be given by the following stochastic format

$$ C_t = \alpha + \beta_0 Y_t + \beta_1 Y_{t-1} + ... + \beta_k Y_{t-k} + u_t $$
Now, we have a whole bunch of multipliers. $\beta_0$ is the short-term impact of $Y_t$ in $C_t$, $\beta_0 + \beta_1$ measures an intermediate impact in $C_t$, so on so forth. The long-run impact is given by $\sum_{i=0}^{k} B_i$, which is the sum of all the different impacts that $\beta_i$ has in $C_t$. 

OK, we now have the dynamics... but how many lags do we choose?

## A Nice Approach

The trick here is to assume that $\beta's$ decline geometrically. If they do, they are tied in time by

$$ \beta_k = \beta_0 \lambda ^ k $$
for $k = 0, 1, 2, ...$, etc. 

This equation it is just saying the more recent the data is, the more relevant is to explain the current changes in $C_t$. In finance, for example, analysts frequently use the [EWMA estimator](https://www.investopedia.com/articles/07/ewma.asp), which explores the same idea.

When 

- $k = 0 \rightarrow \beta_0 \approx \beta_0$ 
- $k = 1 \rightarrow \beta_1 \approx \beta_0 \lambda$
- $k = 2 \rightarrow \beta_2 \approx \beta_0 \lambda ^ 2$
- $...$
- $k = n \rightarrow \beta_n \approx \beta_0 \lambda ^ n$

Since we have defined the $\beta's$ as a geometric sequence, their limiting behavior is known and converge to

$$\lim_{k \to \infty} = \beta_0 \bigg( \frac{1}{1 - \lambda} \bigg)$$
Writing the consumption function using the relations derived above yields 

$$ C_t = \alpha + \beta_0 Y_t + \beta_0 \lambda Y_{t-1} + \beta_0 \lambda^2 Y_{t-2} + ... + \beta_0 \lambda^k Y_{t-k} + u_t $$
What is valid for $C_t$ must also valid for $C_{t-1}$ and we are allowed to rewrite $C$ as

$$ C_{t-1} = \alpha + \beta_0 Y_{t-1} + \beta_0 \lambda Y_{t-2} + \beta_0 \lambda^2 Y_{t-3} + ... + \beta_0 \lambda^k Y_{t-k-1} + u_{t-1} $$
The elegant move comes from the subtraction of $C_t$ by $\lambda C_{t-1}$, that is: $C_t - \lambda C_{t-1}$. This algebra leads to

$$ C_t = \alpha(1 - \lambda) + \beta_0 Y_t + \lambda C_{t-1} + v_t $$
in which $v_t = u_t - \lambda u_{t-1}$.

_Voil√†_! A process that appeared to be quite complicated is now greatly simplified! 

The equation above shows the short-them response of $C_t$ for an increase in $Y_t$. More importantly, even if we know that the effect of $Y$ in $C$ occur with lags, the whole adjustment path can be recovered by $C_{t-1}$ and the follow dependencies

- $t=0 \rightarrow \beta_0 \approx \beta_0$ 
- $t=1 \rightarrow \beta_1 \approx \beta_0 \lambda$
- $t=2 \rightarrow \beta_2 \approx \beta_0 \lambda ^2$
- $t=3 \rightarrow \beta_3 \approx \beta_0 \lambda ^3$
- $...$

The long-term relationship, on the other hand, can be retrieved dividing the right-hand side of $C_t$ by $(1 - \lambda)$ and excluding $C_{t-1}$ from the equation (equilibrium definition implies $C_t = C_{t-1}$ in long-run). 

Finally, the most important feature of this _ad hoc_ approach is the connection with the [Adaptative Expectations Hypothesis](https://www.investopedia.com/terms/a/adaptiveexpthyp.asp). 


## Adaptative Expectations

Assume that individuals change their consumption behavior only if their are confident that changes they face are permanent. If this is not the case, switches are short-lived. In this universe, a long-lasting stir would only be sustained by long-run movements of the state of the economy (a new equilibrium condition), not by current noise. [Milton Friedman](https://www.nobelprize.org/prizes/economic-sciences/1976/friedman/facts/) was the first to raise this point in great detail (see [here](https://press.princeton.edu/titles/978.html)).

A theoretical model to test this proposition should look more or less like

$$ C_t = \alpha + \beta_0 Y^*_t + u_t $$
in which $Y^*$ is the long-run output and $u_t$ is a _white-noise_ disturbance. 

Since _$Y^*$ is non-observable_, let's infer, as a starting point, that individuals can (and do) learn from their experiences. A parsimonious way to incorporate this behavior is by setting

$$ Y^*_t - Y^*_{t-1} = \gamma (Y_t - Y^*_{t-1}) $$
which is identical to $Y^*_t = \gamma Y_t + (1 -\gamma) Y^*_{t-1}$. 

The equilibrium output is a weighted averaged of the current output and the first lag of its long-term (equilibrium) value. In this case, $\gamma$ and $1 - \gamma$ act as weights. 

Substituting $Y^*_t$ in $C_t$ and collecting the terms produce 

$$ C_t = \gamma \alpha + \gamma \beta_0 Y_t + (1 - \gamma) C_{t-1} + v_t $$
in which $v_t = u_t - (1 - \gamma) u_{t-1}$.

_This virtually identical to the equation derived before assuming a geometric decay for $\beta$_!


## A Theorical Note

The derivation of Ordinary Least Squares (OLS) adds a restriction on the residuals behavior: they should not be correlated with any of the explanatory variables. As a consequence, the autocorrelation between $v_t$ and $v_{t-1}$ is null (or close to zero). 

Unfortunately, that's not the case for our model. Note:

$$
\begin{align}
Cov(v_t, v_{t-1}) &= \big[u_t - (1 - \gamma) u_{t-1} \big] \big[u_{t-1} - (1 - \gamma) u_{t-2} \big] \\
Cov(v_t, v_{t-1}) &= -(1 - \gamma) u_{t-1}^2 \\
Cov(v_t, v_{t-1}) &= -(1 - \gamma) \sigma ^2 
\end{align}
$$

The autocorrelation does not sum to zero. The result is identical for $C_{t-1}$ and $v_t$:

$$
\begin{align}
Cov(C_{t-1}, v_t) &= \big [C_{t-1} \big] \big[u_{t} - (1 - \gamma) u_{t-1} \big] \\
Cov(C_{t-1}, v_t) &= \big [\gamma \alpha + \gamma \beta_0 Y_{t-1} + (1 - \gamma) C_{t-2} + u_{t-1} - (1 - \gamma) u_{t-2} \big] \big[u_{t} - (1 - \gamma) u_{t-1} \big] \\
Cov(C_{t-1}, v_t) &= -(1 - \gamma) u_{t-1}^2  \\
Cov(C_{t-1}, v_t) &= -(1 - \gamma) \sigma ^2 
\end{align}
$$

Therefore, $C_{t-1}$, although predetermined, is not fully exogenous. As a result, it's not possible to rule out, without further investigation, the possibility of biased and inconsistent estimators (whether or not this is the case, however, is an empirical question). 

Fortunately, this obstacle can be addressed by the [instrumental variables (IV)](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) approach. 

In the following session we shall use both (OLS and IV). 


## Empirical Estimation of $C_t$

Let's assume that the _true_ specification of $C_t$ is given by $C_t = \alpha + \beta_0 Y^*_t + u_t$ with adaptive expectations. 

From the steps derived above we have to estimate 

$$ C_t = \alpha(1 - \lambda) + \beta_0 Y_t + \lambda C_{t-1} + v_t $$
Let's do that step-by-step using [RStudio](https://www.rstudio.com/).

#### 1. Load libraries

The required libraries for this exercise are

```{r, message=FALSE, warning=FALSE, eval=FALSE}

library(tidyverse)   # loads the tidyverse
library(timetk)      # tidy time manipulation
library(tibbletime)  # tidy time manipulation
library(sidrar)      # download data from IBGE
library(seasonal)    # seasonal decomposition
library(lmtest)      # robust standard errors and IV estimation
library(sandwich)    # robust stardard errors
library(strucchange) # stability tests

```


If you don't have any of them installed yet, just run `install.packages('desired package')` into your console. All libraries are available at [CRAN](https://cran.r-project.org/).

#### 2. Get the data

The following data are needed:

- Consumption
- GDP net of taxes (a _proxy_ for disposable income)
- Consumer Inflation Index (IPCA)

The choice for first two is obvious. The last one, however, has a different purpose: is used to get rid of distortionary effects caused by inflation. The period covered will go from 2003 up to 3Q of 2018 (last data available). 

All the data will be downloaded from [IBGE](https://ww2.ibge.gov.br/english/), the official provider of brazilian national accounts. _Consumption_, _GDP_ and _Taxes_ are computed on a quarterly basis, while consumer inflation is reported monthly. The cumulative inflation over three months will be used to make easier to aggregate the data from a frequency of 12 to 4 periods (we just have to select the 3, 6, 9 and 12 observations of each year). With the quarterly cumulative inflation index at hand, deflate $C$ and $Y^d$ is painless.

The package [sidrar](https://cran.r-project.org/web/packages/sidrar/index.html) is used for the downloads (vignette [here](https://cran.r-project.org/web/packages/sidrar/vignettes/Introduction_to_sidrar.html)).

*** 

__A note:__ It's possible to calculate $Y^d$ from brazilian national accounts, but it's not an easy task given the high level of government transfers and "unconventional" taxation methods adopted by official authorities (see [here](http://www.brazil-help.com/taxes.htm)). As so, $Y - T$ is used as a _proxy_ for $Y^d$.

***

```{r, message=FALSE, warning=FALSE}

# consumption
consumption <- get_sidra(
    api = '/t/1846/n1/all/v/all/p/last%2063/c11255/93404/d/v585%200'
    ) %>% 
    as_tibble() %>% 
    select(consumption = Valor)

# gdp
gdp <- get_sidra(
    api = '/t/1846/n1/all/v/all/p/last%2063/c11255/90707/d/v585%200'
    ) %>% 
    as_tibble() %>% 
    select(gdp = Valor)

# taxes
taxes <- get_sidra(
    api = '/t/1846/n1/all/v/all/p/last%2063/c11255/90706/d/v585%200'
    ) %>% 
    as_tibble() %>% 
    select(taxes = Valor)

# inflation
ipca <- get_sidra(
    api = '/t/1737/n1/all/v/2263/p/all/d/v2263%202'
    ) %>% 
    as_tibble() %>% 
    separate(col = 'M√™s', into = c('month', 'year')) %>% 
    mutate(year = as.numeric(year)) %>% 
    filter(month %in% c('mar√ßo', 'junho', 'setembro', 'dezembro'), year >= 2003) %>% 
    slice(-nrow(.)) %>% 
    select(ipca = Valor)

```


#### 3. Dessazonalize and Deflate

Those are "raw" objects and, not by surprise, exhibit a strong seasonal component (evidence omitted to save space). There are different ways of dealing with this issue and I think that, at least among economists, the most used technique is X-13 ARIMA-SEATS. This is the same procedure adopted by [US Census Bureau](https://www.census.gov/srd/www/x13as/). I will cherish the tradition among my peers by using it as well. 

The seasonally adjusted data have very different magnitudes. To make them more comparable, the year of 2003 is chosen as base year after the series being deflated. The base year is an index starting at 100.

The script bellow show how it is done.

```{r,  message=FALSE, warning=FALSE}

data_clean <- 
    
    # add dates
    create_series('2003' ~ '2018-09', period = 'quarterly') %>% 
    
    # bind all the series
    bind_cols(consumption, gdp, taxes, ipca / 100 + 1) %>% 
    
    # tidy thing
    gather(key = key, value = value, -date) %>% 
    mutate_if(is_character, as_factor) %>%
    group_by(key) %>%
    nest(date, value) %>%
    
    # seasonal package only operates with the ts class
    mutate(map(
        .x = data, 
        
        # coerce to ts
        .f = ~ tk_ts(
            data      = .x, 
            start     = c(2003, 1), 
            end       = c(2018, 3), 
            frequency = 4, 
            silent = TRUE
        ) %>% 
            
            # then apply the seasonal filtering
            seas() %>% 
            
            # extract the fitted values
            final() %>% 
            
            # and bring back to the tidy framework
            tk_tbl() %>% 
            rename(value_seats = value)
        )
    ) %>% 
    unnest() %>% 
    select(date, key, everything(), -index, -value) %>% 
    
    spread(key, value_seats) %>% 
    
    # deflate
    mutate(
        consumption = consumption / ipca, 
        gdp         = gdp / ipca, 
        taxes       = taxes / ipca, 
        y_d         = gdp - taxes,
        
        # 2003 as a base year   
        consumption   = (consumption / consumption[[1]]) * 100, 
        y_d           = (y_d / y_d[[1]]) * 100
        ) %>% 
    select(date, consumption, y_d) 

data_clean

```


#### 4. Fit OLS

With a clean `tibble` at hand let's jump straight to OLS estimation:

```{r, message=FALSE, warning=FALSE}

model_ols <- data_clean %>% 
    lm(consumption ~ y_d + lag(consumption, 1), data = .) 

model_ols %>% 
    coeftest(x = ., vcov. = sandwich::vcovHAC)

```

Except for the intercept, all the estimated coefficients have the expected sign. They are also highly significant, as can be seen by the calculated t-statistic (the [HAC](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors) estimator has already corrected the standard errors for autocorrelation and heteroskedasticity). From the regression we find

$$
\begin{align}
\alpha(1 - \lambda) &= -1.018853 \\
\beta_0             &= 0.321349  \\
\lambda             &= 0.692202  \\
\end{align}
$$

The short-term impact of a positive one-unit shock in the GDP (net of taxes) is of the magnitude of $0.33(=\beta_0)$. In the period that follow, the shock is still in effect and have an impact of $\beta_o \lambda$, in $t = 2$; $\beta_o \lambda ^ 2$, in $t = 3$; so on so forth.

The short and the long-term equations are rewritten as

$$ 
\begin{align}
C_t &= -1.02 + 0.32 Y^d_t + 0.70 C_{t-1} & \text{(short-term)} \\
C_t &= -3.31 + 1.04 Y^d_t                & \text{(long-term)} 
\end{align}
$$
Notice the difference in _marginal propensity to consume_ as we allow the time window to expand. In the long-run, a positive unit shock in $Y^d$ translates entirely into new consumption. 

Since this process is a geometric sequence, the average adjustment can be calculated as

$$ \text{Avg}_{ols} = \frac{\lambda}{1 - \lambda} = \frac{0.692202}{1 - 0.692202} \approx 2.25 \text{ quarters}$$
which is a reasonably fast transition. 

Against the initial expectations, there is no evidence of autocorrelation up to 12 lags, according to Breusch-Godfrey Test:

```{r}

data_clean %>% 
    bgtest(formula = consumption ~ y_d + lag(consumption, 1), 
           order   = 12, 
           type    = 'F', 
           data    = ., 
           fill    = 100) 

```

Finally, the fitted parameters appears to be stable and don't cross the barriers of the CUSUM test.

```{r}

data_clean %>% 
    efp(formula = consumption ~ y_d + lag(consumption, 1), 
        data    = ., 
        type    = 'OLS-CUSUM', 
        dynamic = FALSE) %>% 
    plot()

```


#### 4. Fit IV

The IV estimation requires a two step procedure: (i) run $C_t = \theta_0 + \theta_1 Y^d_t$ and save $\hat{C}_{t}$; (ii) run $C_t = \alpha + \beta Y^d_t + \Theta \hat{C}_{t-1}$, in which $\hat{C}_{t}$ is now "purified" from it's improper correlation with $v_t$.

```{r}

model_iv <- data_clean %>% 
    mutate(iv = lm(consumption ~ y_d, data = .)$fitted) %>% 
    lm(consumption ~ y_d + lag(iv, 1), data = .)

model_iv %>% 
    coeftest(x = ., vcov. = sandwich::vcovHAC)

```

All the coefficients have the expected sign (including the intercept). The fitted parameter for $Y^d$ is lower than the OLS method and the lagged value for $\hat{C}_{t-1}$ is higher, which may indicate that shocks (when occur) dissipate a at a slower rate.

The regression analysis imply

$$ 
\begin{align}
C_t &= 0.13 + 0.27 Y^d_t + 0.74 C_{t-1} & \text{(short-term)} \\
C_t &= 0.50 + 1.04 Y^d_t                & \text{(long-term)} 
\end{align}
$$
The long-term effect is nearly identical, whereas the short-term responses differs somehow. The average impulse has increased from $2.25$ to $\lambda / (1 - \lambda) = 2.90$ quarters. 

The current model contains autocorrelation (tested using Breusch-Godfrey procedure up to 12 lags), which may not be a severe problem in this case, since the fitted parameters do not deviate much from our previous model. 

As an exercise, let's incorporate the error term into the IV equation, lagged by one period. By doing this, we find:

```{r}

model_iv_lag_error <- data_clean %>% 
    mutate(
        iv = lm(consumption ~ y_d, data = .)$fitted, 
        v  = lm(consumption ~ y_d, data = .)$residuals
        ) %>% 
    lm(consumption ~ y_d + lag(iv, 1) + lag(v, 1), data = .) 

model_iv_lag_error %>% 
    coeftest(x = ., vcov. = sandwich::vcovHAC) 

```

The fitted coefficient for $Y^d$ has gone up from $0.27$ (OLS) to $0.43$. That's a huge change. All the variables are statistically significant at 95% confidence interval. 

The behavior equations are

$$ 
\begin{align}
C_t &= -2.75 + 0.43 Y^d_t + 0.58 C_{t-1} & \text{(short-term)} \\
C_t &= -6.57 + 1.04 Y^d_t                & \text{(long-term)} 
\end{align}
$$
This IV representation appears to be well specified and passes all tests applied so far. The Breusch-Godfrey test (12 lags) has a p-value of $0.98$ (not shown here to save space). As so, the null-hypothesis of no autocorrelation can't be rejected. The OLS-CUSUM points in the same direction by showing stability in the coefficients.

```{r}

data_clean %>% 
    mutate(
        iv = lm(consumption ~ y_d, data = .)$fitted, 
        v  = lm(consumption ~ y_d, data = .)$residuals
    ) %>% 
    efp(formula = consumption ~ y_d + lag(iv, 1) + lag(v, 1), 
        data    = ., 
        type    = 'OLS-CUSUM', 
        dynamic = FALSE) %>% 
    plot()

```


## A Graphical Interpretation

Let's use the objects created above to visualize the impact of a $Y^d_t$ shock in $C_t$.

```{r}

tibble(
    Time = 1:12, 
    OLS  = model_ols$coefficients[[2]] * ((model_ols$coefficients[[3]]) ^ Time) / model_ols$coefficients[[3]], 
    IV = model_iv$coefficients[[2]] * ((model_iv$coefficients[[3]]) ^ Time) / model_iv$coefficients[[3]], 
    `IV with lag error` = model_iv_lag_error$coefficients[[2]] * ((model_iv_lag_error$coefficients[[3]]) ^ Time) / model_iv_lag_error$coefficients[[3]]
    ) %>% 
    
    # tidy
    gather(key = "Models", value = "Shock", -Time, factor_key = TRUE) %>% 
    
    # plot
    ggplot(aes(x = Time, y = Shock, color = Models)) +
    geom_line(size = 1) +
    scale_colour_viridis_d() + 
    labs(title    = 'Effect of a Temporary Shock in Consumption', 
         subtitle = 'Period 2003-2018. Quarterly data.', 
         y        = 'Impact (%)', 
         caption  = "Source: IBGE and Brazilian National Accounts.") + 
    theme_minimal() + 
    theme(legend.position = "bottom")

```

It clearly stands out that the IV Model (with lag error) manifests a higher short-term impact then the rest of the models. At the same time, the shocks estimated with this specification seems to decay faster then the rest. After two years (8 quarters), only a minor fraction of previous disturbances still affect the consumption path. This result is valid for all models.

From the equations above it is also possible to draw a graph of $C-Y^d$ in the two dimensional hyperplane. 

```{r}

sequence <- seq(from = 100, to = 500, by = 10)

list(
    tibble(
        seq   = sequence,
        fit   = ‚àí1.02 + 0.32 * sequence,
        model = 'OLS', 
        Type  = 'Short-Run'
    ), 
    tibble(
        seq   = sequence,
        fit   = ‚àí3.31 + 1.04 * sequence,
        model = 'OLS', 
        Type  = 'Long-Run'
    ), 
    tibble(
        seq   = sequence,
        fit   = ‚àí0.13 + 0.27 * sequence,
        model = 'IV', 
        Type  = 'Short-Run'
    ),
    tibble(
        seq   = sequence,
        fit   = 0.50  + 1.04 * sequence,
        model = 'IV', 
        Type  = 'Long-Run'
    ),
    tibble(
        seq   = sequence,
        fit   = ‚àí2.75 + 0.43 * sequence,
        model = 'IV With Lag Error', 
        Type  = 'Short-Run'
    ),
    tibble(
        seq   = sequence,
        fit   = -6.57 + 1.04 * sequence,
        model = 'IV With Lag Error', 
        Type  = 'Long-Run'
    )
) %>% 
    reduce(bind_rows) %>% 
    mutate_if(is_character, as_factor) %>% 
    ggplot(aes(x = seq, y = fit, color = Type)) +
    facet_wrap(~model, scales = 'free_y') + 
    geom_line(size = 1) + 
    scale_colour_viridis_d() + 
    theme_minimal() + 
    theme(legend.position = "bottom") + 
    labs(title    = 'Brazilian Consumption Function',
         subtitle = 'Period of 2003-2018. Quarterly data.',
         x = 'Disposable Income', 
         y = 'Consumption', 
         caption = "Source: IBGE and Brazilian National Accounts.")
    


```


## Conclusions

In this post three different specifications were discussed to explain the brazilian consumption path. In agreement with the theory (see [Romer](https://www.amazon.com/Advanced-Macroeconomics-Mcgraw-hill-Economics-David/dp/1260185214/ref=sr_1_1?ie=UTF8&qid=1550107391&sr=8-1&keywords=advanced+macroeconomics+romer) chapter 8), the relationship between consumption and available income is flatter in the short-run and stepper for longer periods of time. There is also some uncertainty around the right parameter for $Y^d$, that lies between $\{0.27, 0.43\}$. 

As we give time for shocks to fully accommodate, a unit change in $Y^d$ implies an impact of 1-to-1 in $C$. The average adjustment occurs relatively fast, around 2 or 3 quarters. Taken together, the results indicate there is a low propensity to forgo consumption, given increases in income. In technical terms: _the intertemporal elasticity of substituition appears to be low_.

It's possible to expand this study in many ways. The inclusion of interest-rates may help to improve the results, since interest-rates can change the relative prices of consumption among the present and future. Credit as a fraction of the GDP, unemployment, local exchange-rate, etc. may also add new insights. All of those variables can be found at the Brazilian Central Bank (BCB) or IBGE webpages. Additional care with endogenity issues is never too much and always advisable.

